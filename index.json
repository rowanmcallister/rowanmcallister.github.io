[{"authors":["admin"],"categories":null,"content":"I am a machine learning manager at the Toyota Research Institute in California focusing on autonomous vehicle planning. Previously, I was a PhD student at the Machine Learning Group with Carl Rasmussen and a postdoc in the Robotic AI and Learning Lab with Sergey Levine, working on probabilistic modeling toward data-efficient reinforcement learning and autonomous vehicle planning.\n","date":1653436800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1653436800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://rowanmcallister.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a machine learning manager at the Toyota Research Institute in California focusing on autonomous vehicle planning. Previously, I was a PhD student at the Machine Learning Group with Carl Rasmussen and a postdoc in the Robotic AI and Learning Lab with Sergey Levine, working on probabilistic modeling toward data-efficient reinforcement learning and autonomous vehicle planning.","tags":null,"title":"Rowan McAllister","type":"authors"},{"authors":["Rowan McAllister","Blake Wulfe","Jean Mercat","Logan Ellis","Sergey Levine","Adrien Gaidon","Sergey Levine"],"categories":null,"content":"","date":1653436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653436800,"objectID":"825e895d99f46fb5298c8e8c1937ad36","permalink":"https://rowanmcallister.github.io/publication/capo/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/capo/","section":"publication","summary":"Autonomous vehicle software is typically structured as a modular pipeline of individual components (e.g., perception, prediction, and planning) to help separate concerns into interpretable sub-tasks. Even when end-to-end training is possible, each module has its own set of objectives used for safety assurance, sample efficiency, regularization, or interpretability. However, intermediate objectives do not always align with overall system performance. For example, optimizing the likelihood of a trajectory prediction module might focus more on easy-to-predict agents than safety-critical or rare behaviors (e.g., jaywalking). In this paper, we present control-aware prediction objectives (CAPOs), to evaluate the downstream effect of predictions on control without requiring the planner be differentiable. We propose two types of importance weights that weight the predictive likelihood, one using an attention model between agents, and another based on control variation when exchanging predicted trajectories for ground truth trajectories. Experimentally, we show our objectives improve overall system performance in suburban driving scenarios using the CARLA simulator.","tags":["Autonomous Vehicles"],"title":"Control-Aware Prediction Objectives for Autonomous Driving","type":"publication"},{"authors":["Blake Wulfe","Ashwin Balakrishna","Logan Ellis","Jean Mercat","Rowan McAllister","Adrien Gaidon"],"categories":null,"content":"","date":1643068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643068800,"objectID":"28472646a894a9e3df7e2e4fa70840c4","permalink":"https://rowanmcallister.github.io/publication/dard/","publishdate":"2022-01-25T00:00:00Z","relpermalink":"/publication/dard/","section":"publication","summary":"The ability to learn reward functions plays an important role in enabling the deployment of intelligent agents in the real world. However, comparing reward functions, for example as a means of evaluating reward learning methods, presents a challenge. Reward functions are typically compared by considering the behavior of optimized policies, but this approach conflates deficiencies in the reward function with those of the policy search algorithm used to optimize it. To address this challenge, Gleave et al. (2020) propose the Equivalent-Policy Invariant Comparison (EPIC) distance. EPIC avoids policy optimization, but in doing so requires computing reward values at transitions that may be impossible under the system dynamics. This is problematic for learned reward functions because it entails evaluating them outside of their training distribution, resulting in inaccurate reward values that we show can render EPIC ineffective at comparing rewards. To address this problem, we propose the Dynamics-Aware Reward Distance (DARD), a new reward pseudometric. DARD uses an approximate transition model of the environment to transform reward functions into a form that allows for comparisons that are invariant to reward shaping while only evaluating reward functions on transitions close to their training distribution. Experiments in simulated physical domains demonstrate that DARD enables reliable reward comparisons without policy optimization and is significantly more predictive than baseline methods of downstream policy performance when dealing with learned reward functions.","tags":["Reinforcement Learning"],"title":"Dynamics-Aware Comparison of Learned Reward Functions","type":"publication"},{"authors":["Rowan McAllister"],"categories":[],"content":" Choosing the right research problem is difficult. Both Vladlen Koltun and Richard Hamming offer some great advice on selecting which research problems to work on and how. This post is simply a summary of Richard\u0026rsquo;s talk \u0026ldquo;You and Your Research\u0026rdquo;, and Vladlen\u0026rsquo;s talk \u0026ldquo;Doing (Good) Research\u0026rdquo;, both worth watching in their entirety!\nProblem Progressing from undergrad into research can seem to flip any STEM field from a science into an art: undergrads identify solutions to problems versus researchers who identify problems worth solving. The right problem is ill-defined, yet critical to identify.\nDefine \u0026ldquo;right\u0026rdquo; Richard: right is simply subjective, up to you.\nVladlen: right should:\n Contribute to the progress and well-being of humanity, should be useful. Maximize your contribution to the research community and society (not vice versa) by  inspiring the community; shifting the way the community thinks about existing problems (or new ones); be a methodical evaluation of methods to demonstrate which should be used when; be a dataset, benchmark, or open source code people find useful.   How to work on the \u0026ldquo;right\u0026rdquo; problems  Create a long term vision to guide yourself (not a random walk), have high-level meta goals.  Ask yourself: what fields or applications do you find interesting or meaningful and why? You\u0026rsquo;ll only really succeed in work that you enjoy. Doing meaningful work helps avoid burnout.  Understand what components are needed for your high-level goals to become a reality. Understand the state-of-the-art for each component by reading and doing (explained later).  Analyze bottlenecks. Understand what you could do to benefit the collective progress of the community.  Solve a component\u0026rsquo;s problem if the time is right.  Too early = not sufficient tools to attack the problem well yet, will make little headway. Too late = you won\u0026rsquo;t contribute to affecting the research community\u0026rsquo;s direction much, improvements will be too incremental.   Understand by reading  Read a lot. Read critically. When reading about a new method, ask:  What are its limitations and assumptions, when will it break? What gaps remain?  Reading critically enables new ways of thinking about methods. Otherwise you won\u0026rsquo;t make big changes, you\u0026rsquo;ll merely extend the old with incremental improvements. So tolerate some subjective uncertainty about the \u0026ldquo;accepted methods\u0026rdquo;. Be sufficiently skeptical to perceive flaws and see what others missed. It\u0026rsquo;s OK to be (justifiably) contrarian! Inform your colleagues about your research interests. They\u0026rsquo;ll give you greater observability of the relevant papers you should read (but missed).  Understand by doing  Re-implementation is the best way to understand a method. You\u0026rsquo;ll discover details you can\u0026rsquo;t glean from just reading, and develop deeper insight. Implementation helps observe things you never set out to discover. And by following the scientific method down this rabbit hole, you can then go about testing assumptions about what generated the effect you observed. Research then begets research: the more things you try, the more you observe, the more things you want to try next etc, the more people will want to collaborate with you, and invite you to things, and tell you about things you didn\u0026rsquo;t otherwise know. Be methodical when (re-)implementing: swap a method\u0026rsquo;s subcomponents in and out in a controlled way to understand which components were critical.  Understand by writing  Write down your ideas + experimental methods early. This makes clear what your assumptions are and what gaps remain to be filled. You can also share this (more concrete) document of your ideas with your peers.  Work ethic  Luck favors the prepared! Research is 99% perspiration, 1% inspiration. Newton thought if people just worked as hard as he did, they\u0026rsquo;d get the same result. Great researchers always think about problems, even when they\u0026rsquo;re away from work. Set aside quiet time reflection time for “great thoughts” like Friday afternoon to:  Think higher-level about where you are heading, is this the right direction? Don\u0026rsquo;t cling to bad ideas too long, know when to walk away / bury work.  Have 10\u0026ndash;20 problems in your head at any one time, so you don\u0026rsquo;t stay \u0026ldquo;stuck\u0026rdquo; in one problem. When a clue comes along for one of them, focus on it. Have an open door policy (need to collaborate), it\u0026rsquo;s better in the long run. Learn how to communicate your ideas in a formal and casual way. Be on the lookout for collaborators. Quality over quantity (do not add noise to conferences or arXiv, or worse: mislead).  Contribution is the goal, the publication process is a liability.  Confidence is necessary: it is important that you believe you can do great work.  Challenging yourself  Ask yourself: if what I\u0026rsquo;m doing is not important, and not likely to lead to important things in the future, why am I working on it?  Caveat: not to say adopt \u0026ldquo;Nobel prize winner syndrome\u0026rdquo; of only working on \u0026ldquo;great problems\u0026rdquo; and getting nowhere. Instead, work on small acorns that have the potential to grow into mighty oaks.  Ask yourself: what are the most important problems in my field? Be willing to accept change, don\u0026rsquo;t stick to a particular method.  Warnings  Sometimes the people around you cannot see you\u0026rsquo;re doing great work.  A printer-friendly version of this post is here.\n","date":1635206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635206400,"objectID":"8db034acdc3224abf38808ba0cf1d04a","permalink":"https://rowanmcallister.github.io/post/research/","publishdate":"2021-10-26T00:00:00Z","relpermalink":"/post/research/","section":"post","summary":"Choosing the *right* research problem + how to succeed","tags":null,"title":"How to do research","type":"post"},{"authors":["Boris Ivanovic","Kuan-Hui Lee","Pavel Tokmakov","Blake Wulfe","Rowan McAllister","Adrien Gaidon","Marco Pavone"],"categories":null,"content":"","date":1619481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619481600,"objectID":"8f458a2b1a1f0cbc4370997c8ae9362d","permalink":"https://rowanmcallister.github.io/publication/haicu/","publishdate":"2021-04-27T00:00:00Z","relpermalink":"/publication/haicu/","section":"publication","summary":"Reasoning about the future behavior of other agents is critical to safe robot navigation. The multiplicity of plausible futures is further amplified by the uncertainty inherent to agent state estimation from data, including positions, velocities, and semantic class. Forecasting methods, however, typically neglect class uncertainty, conditioning instead only on the agent’s most likely class, even though perception models often return full class distributions. To exploit this information, we present HAICU, a method for heterogeneous-agent trajectory forecasting that explicitly incorporates agents’ class probabilities. We additionally present PUP, a new challenging real-world autonomous driving dataset, to investigate the impact of Perceptual Uncertainty in Prediction. It contains challenging crowded scenes with unfiltered agent class probabilities that reflect the long-tail of current state-of-the-art perception systems. We demonstrate that incorporating class probabilities in trajectory forecasting significantly improves performance in the face of uncertainty, and enables new forecasting capabilities such as counterfactual predictions.","tags":["Autonomous Vehicles"],"title":"Heterogeneous-Agent Trajectory Forecasting Incorporating Class Uncertainty","type":"publication"},{"authors":["Nicholas Rhinehart","Jeff He","Charles Packer","Matthew Wright","Rowan McAllister","Joseph Gonzalez","Sergey Levine"],"categories":null,"content":"","date":1618963201,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618963201,"objectID":"33769b280296f33a6a29a3d139c7d660","permalink":"https://rowanmcallister.github.io/publication/contingency/","publishdate":"2021-04-21T00:00:01Z","relpermalink":"/publication/contingency/","section":"publication","summary":"Humans have a remarkable ability to make decisions by accurately reasoning about future events, including the future behaviors and states of mind of other agents. Consider driving a car through a busy intersection, it is necessary to reason about the physics of the vehicle, the intentions of other drivers, and their beliefs about your own intentions. If you signal a turn, another driver might yield to you, or if you enter the passing lane, another driver might decelerate to give you room to merge in front. Competent drivers must plan how they can safely react to a variety of potential future behaviors of other agents before they make their next move. This requires contingency planning, explicitly planning a set of conditional actions that depend on the stochastic outcome of future events. Contingency planning outputs a policy that is a function of future timesteps and observations, whereas standard model predictive control-based planning outputs a sequence of future actions, which is equivalent to a policy that is only a function of future timesteps. In this work, we develop a general-purpose contingency planner that is learned end-to-end using high-dimensional scene observations and low-dimensional behavioral observations. We use a conditional autoregressive flow model to create a compact contingency planning space, and show how this model can tractably learn contingencies from behavioral observations. We developed a closed-loop control benchmark of realistic multi-agent scenarios in a driving simulator (CARLA), on which we compare our method to various noncontingent methods that reason about multi-agent future behavior, including several state-of-the-art deep learning-based planning approaches. We illustrate that these noncontingent planning methods fundamentally fail on this benchmark, and find that our deep contingency planning method achieves significantly superior performance.","tags":["Autonomous Vehicles","Planning"],"title":"Contingencies from Observations: Tractable Contingency Planning with Learned Behavior Models","type":"publication"},{"authors":["Tim Rudner","Vitchyr Pong","Rowan McAllister","Yarin Gal","Sergey Levine"],"categories":null,"content":"","date":1618963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618963200,"objectID":"82e24aa42db734293b21b3a81f1dddd0","permalink":"https://rowanmcallister.github.io/publication/goals/","publishdate":"2021-04-21T00:00:00Z","relpermalink":"/publication/goals/","section":"publication","summary":"While reinforcement learning algorithms provide automated acquisition of optimal policies, practical application of such methods requires a number of design decisions, such as manually designing reward functions that not only define the task, but also provide sufficient shaping to accomplish it. In this paper, we discuss a new perspective on reinforcement learning, recasting it as the problem of inferring actions that achieve desired outcomes, rather than a problem of maximizing rewards. To solve the resulting outcome-directed inference problem, we establish a novel variational inference formulation that allows us to derive a well-shaped reward function which can be learned directly from environment interactions. From the corresponding variational objective, we also derive a new probabilistic Bellman backup operator reminiscent of the standard Bellman backup operator and use it to develop an off-policy algorithm to solve goal-directed tasks. We empirically demonstrate that this method eliminates the need to design reward functions and leads to effective goal-directed behaviors.","tags":["Reinforcement Learning"],"title":"Outcome-Driven Reinforcement Learning via Variational Inference","type":"publication"},{"authors":["Angelos Filos","Panagiotis Tigas","Rowan McAllister","Nicholas Rhinehart","Sergey Levine","Yarin Gal"],"categories":null,"content":"","date":1593129600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593129600,"objectID":"f76a56afb594d0a3f687c1bb0a09c6a7","permalink":"https://rowanmcallister.github.io/publication/carnovel/","publishdate":"2020-06-26T00:00:00Z","relpermalink":"/publication/carnovel/","section":"publication","summary":"Out-of-training-distribution (OOD) scenarios are a common challenge of learning agents at deployment, typically leading to arbitrary deductions and poorly-informed decisions. In principle, detection of and adaptation to OOD scenes can mitigate their adverse effects. In this paper, we highlight the limitations of current approaches to novel driving scenes and propose an epistemic uncertainty-aware planning method, called robust imitative planning (RIP). Our method can detect and recover from some distribution shifts, reducing the overconfident and catastrophic extrapolations in OOD scenes. If the model's uncertainty is too great to suggest a safe course of action, the model can instead query the expert driver for feedback, enabling sample-efficient online adaptation, a variant of our method we term adaptive robust imitative planning (AdaRIP). Our methods outperform current state-of-the-art approaches in the nuScenes prediction challenge, but since no benchmark evaluating OOD detection and adaption currently exists to assess control, we introduce an autonomous car novel-scene benchmark, CARNOVEL, to evaluate the robustness of driving agents to a suite of tasks with distribution shifts.","tags":["Autonomous Vehicles"],"title":"Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?","type":"publication"},{"authors":["Amy Zhang","Rowan McAllister","Roberto Calandra","Yarin Gal","Sergey Levine"],"categories":null,"content":"","date":1592438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592438400,"objectID":"26c6495452d6301b8d38e8514d2f8d2a","permalink":"https://rowanmcallister.github.io/publication/dbc/","publishdate":"2020-06-18T00:00:00Z","relpermalink":"/publication/dbc/","section":"publication","summary":"We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that both provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.","tags":["Reinforcement Learning"],"title":"Learning Invariant Representations for Reinforcement Learning without Reconstruction","type":"publication"},{"authors":["Suneel Belkhale","Rachel Li","Gregory Kahn","Rowan McAllister","Roberto Calandra","Sergey Levine"],"categories":null,"content":"","date":1587600000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587600000,"objectID":"f249819bd4b58080e78a19725f59b8e3","permalink":"https://rowanmcallister.github.io/publication/meta-mbrl/","publishdate":"2020-04-23T00:00:00Z","relpermalink":"/publication/meta-mbrl/","section":"publication","summary":"Transporting suspended payloads is challenging for autonomous aerial vehicles because the payload can cause significant and unpredictable changes to the robot's dynamics. These changes can lead to suboptimal flight performance or even catastrophic failure. Although adaptive control and learning-based methods can in principle adapt to changes in these hybrid robot-payload systems, rapid mid-flight adaptation to payloads that have a priori unknown physical properties remains an open problem. We propose a meta-learning approach that ``learns how to learn'' models of altered dynamics within seconds of post-connection flight data. Our experiments demonstrate that our online adaptation approach outperforms non-adaptive methods on a series of challenging suspended payload transportation tasks.","tags":["Reinforcement Learning"],"title":"Model-Based Meta-Reinforcement Learning forFlight with Suspended Payloads","type":"publication"},{"authors":["Brijen Thananjeyan","Ashwin Balakrishna","Ugo Rosolia","Felix Li","Rowan McAllister","Joseph Gonzalez","Sergey Levine","Francesco Borrelli","Ken Goldberg"],"categories":null,"content":"","date":1559260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559260800,"objectID":"64538f234b1030a7d33cabe80e59706b","permalink":"https://rowanmcallister.github.io/publication/saved/","publishdate":"2019-05-31T00:00:00Z","relpermalink":"/publication/saved/","section":"publication","summary":"Reinforcement learning (RL) for robotics is challenging due to the difficulty in hand-engineering a dense cost function, which can lead to unintended behavior, and dynamical uncertainty, which makes it hard to enforce constraints during learning. We address these issues with a new model-based reinforcement learning algorithm, safety augmented value estimation from demonstrations (SAVED), which uses supervision that only identifies task completion and a modest set of suboptimal demonstrations to constrain exploration and learn efficiently while handling complex constraints. We derive iterative improvement guarantees for SAVED under known stochastic nonlinear systems. We then compare SAVED with 3 state-of-the-art model-based and model-free RL algorithms on 6 standard simulation benchmarks involving navigation and manipulation and 2 real-world tasks on the da Vinci surgical robot. Results suggest that SAVED outperforms prior methods in terms of success rate, constraint satisfaction, and sample efficiency, making it feasible to safely learn complex maneuvers directly on a real robot in less than an hour. For tasks on the robot, baselines succeed less than 5% of the time while SAVED has a success rate of over 75% in the first 50 training iterations.","tags":null,"title":"Safety Augmented Value Estimation from Demonstrations (SAVED): Safe Deep Model-Based RL for Sparse Cost Robotic Tasks","type":"publication"},{"authors":["Nicholas Rhinehart","Rowan McAllister","Kris Kitani","Sergey Levine"],"categories":null,"content":"","date":1557187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557187200,"objectID":"4314b09673cf171f2a6de1d76368f903","permalink":"https://rowanmcallister.github.io/publication/precog/","publishdate":"2019-05-07T00:00:00Z","relpermalink":"/publication/precog/","section":"publication","summary":"Forecasting the motion of multiple interacting vehicles. When one is autonmous, conditioning on its goals helps better-predict the motions of other vehicles.","tags":["Autonomous Vehicles"],"title":"PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings","type":"publication"},{"authors":["Rowan McAllister","Gregory Kahn","Jeff Clune","Sergey Levine"],"categories":null,"content":"","date":1545868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545868800,"objectID":"392cad5eb99b10f7e9a745b76f7adc1e","permalink":"https://rowanmcallister.github.io/publication/ood/","publishdate":"2018-12-27T00:00:00Z","relpermalink":"/publication/ood/","section":"publication","summary":"Deep learning provides a powerful tool for machine perception when the observations resemble the training data. However, real-world robotic systems must react intelligently to their observations even in unexpected circumstances. This requires a system to reason about its own uncertainty given unfamiliar, out-of-distribution observations. Approximate Bayesian approaches are commonly used to estimate uncertainty for neural network predictions, but can struggle with out-of-distribution observations. Generative models can in principle detect out-of-distribution observations as those with a low estimated density. However, the mere presence of an out-of-distribution input does not by itself indicate an unsafe situation. In this paper, we present a method for uncertainty-aware robotic perception that combines generative modeling and model uncertainty to cope with uncertainty stemming from out-of-distribution states. Our method estimates an uncertainty measure about the model's prediction, taking into account an explicit (generative) model of the observation distribution to handle out-of-distribution inputs. This is accomplished by probabilistically projecting observations onto the training distribution, such that out-of-distribution inputs map to uncertain in-distribution observations, which in turn produce uncertain task-related predictions, but only if task-relevant parts of the image change. We evaluate our method on an action-conditioned collision prediction task with both simulated and real data, and demonstrate that our method of projecting out-of-distribution observations improves the performance of four standard Bayesian and non-Bayesian neural network approaches, offering more favorable trade-offs between the proportion of time a robot can remain autonomous and the proportion of impending crashes successfully avoided.","tags":["Reinforcement Learning"],"title":"Robustness to Out-of-Distribution Inputs via Task-Aware Generative Uncertainty","type":"publication"},{"authors":["Kurtland Chua","Roberto Calandra","Rowan McAllister","Sergey Levine"],"categories":null,"content":"","date":1541116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541116800,"objectID":"9eb0854654ecbd931b95d5384b219ab5","permalink":"https://rowanmcallister.github.io/publication/pets/","publishdate":"2018-11-02T00:00:00Z","relpermalink":"/publication/pets/","section":"publication","summary":"Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).","tags":["Reinforcement Learning"],"title":"Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models","type":"publication"},{"authors":["Nicholas Rhinehart","Rowan McAllister","Sergey Levine"],"categories":null,"content":"","date":1539561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539561600,"objectID":"24431e94f69a8cbd95a0bb1775cb5c7c","permalink":"https://rowanmcallister.github.io/publication/dim/","publishdate":"2018-10-15T00:00:00Z","relpermalink":"/publication/dim/","section":"publication","summary":"Imitation Learning (IL) is an appealing approach to learn desirable autonomous behavior. However, directing IL to achieve arbitrary goals is difficult. In contrast, planning-based algorithms use dynamics models and reward functions to achieve goals. Yet, reward functions that evoke desirable behavior are often difficult to specify. In this paper, we propose \"Imitative Models\" to combine the benefits of IL and goal-directed planning. Imitative Models are probabilistic predictive models of desirable behavior able to plan interpretable expert-like trajectories to achieve specified goals. We derive families of flexible goal objectives, including constrained goal regions, unconstrained goal sets, and energy-based goals. We show that our method can use these objectives to successfully direct behavior. Our method substantially outperforms six IL approaches and a planning-based approach in a dynamic simulated autonomous driving task, and is efficiently learned from expert demonstrations without online data collection.  We also show our approach is robust to poorly-specified goals, such as goals on the wrong side of the road.","tags":["Autonomous Vehicles"],"title":"Deep Imitative Models for Flexible Inference, Planning, and Control","type":"publication"},{"authors":["Rowan McAllister","Carl Rasmussen"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"af2caa51d9b47076b5554063df582d0c","permalink":"https://rowanmcallister.github.io/publication/fpilco/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/fpilco/","section":"publication","summary":"We present a data-efficient reinforcement learning method for continuous stateaction systems under significant observation noise. Data-efficient solutions under small noise exist, such as PILCO which learns the cartpole swing-up task in 30s. PILCO evaluates policies by planning state-trajectories using a dynamics model. However, PILCO applies policies to the observed state, therefore planning in observation space. We extend PILCO with filtering to instead plan in belief space, consistent with partially observable Markov decisions process (POMDP) planning. This enables data-efficient learning under significant observation noise, outperforming more naive methods such as post-hoc application of a filter to policies optimised by the original (unfiltered) PILCO algorithm. We test our method on the cartpole swing-up task, which involves nonlinear dynamics and requires nonlinear control.","tags":["Reinforcement Learning","Planning"],"title":"Data-Efficient Reinforcement Learning in Continuous State-Action Gaussian-POMDPs","type":"publication"},{"authors":["Rowan McAllister","Yarin Gal","Alex Kendall","Mark van der Wilk","Amar Shah","Roberto Cipolla","Adrian Weller"],"categories":null,"content":"","date":1503100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503100800,"objectID":"5147407aa40769f729df444826aa7125","permalink":"https://rowanmcallister.github.io/publication/avsafety/","publishdate":"2017-08-19T00:00:00Z","relpermalink":"/publication/avsafety/","section":"publication","summary":"Autonomous vehicle (AV) software is typically composed of a pipeline of individual components, linking sensor inputs to motor outputs. Erroneous component outputs propagate downstream, hence safe AV software must consider the ultimate effect of each component's errors. Further, improving safety alone is not sufficient. Passengers must also *feel* safe to trust and use AV systems. To address such concerns, we investigate three under-explored themes for AV research; safety, interpretability, and compliance. *Safety* can be improved by quantifying the uncertainties of component outputs and propagating them forward through the pipeline. *Interpretability* is concerned with explaining what the AV observes and why it makes the decisions it does, building reassurance with the passenger. *Compliance* refers to maintaining some control for the passenger. We discuss open challenges for research within these themes. We highlight the need for concrete evaluation metrics, propose example problems, and highlight possible solutions.","tags":["Autonomous Vehicles"],"title":"Concrete Problems for Autonomous Vehicle Safety: Advantages of Bayesian Deep Learning","type":"publication"},{"authors":["Rowan McAllister"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"6aad632eb6eb4d81dc799d4ff1bf17a6","permalink":"https://rowanmcallister.github.io/publication/phd/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/phd/","section":"publication","summary":"Applications to learn control of unfamiliar dynamical systems with increasing autonomy are ubiquitous. From robotics, to finance, to industrial processing, autonomous learning helps obviate a heavy reliance on experts for system identification and controller design. Often real world systems are nonlinear, stochastic, and expensive to operate (e.g. slow, energy intensive, prone to wear and tear). Ideally therefore, nonlinear systems can be identified with minimal system interaction. This thesis considers data efficient autonomous learning of control of nonlinear, stochastic systems. Data efficient learning critically requires probabilistic modelling of dynamics. Traditional control approaches use deterministic models, which easily overfit data, especially small datasets. We use probabilistic Bayesian modelling to learn systems from scratch, similar to the PILCO algorithm, which achieved unprecedented data efficiency in learning control of several benchmarks. We extend PILCO in three principle ways. First, we learn control under significant observation noise by simulating a filtered control process using a tractably analytic framework of Gaussian distributions. In addition, we develop the ‘latent variable belief Markov decision process’ when filters must predict under real-time constraints. Second, we improve PILCO’s data efficiency by directing exploration with predictive loss uncertainty and Bayesian optimisation, including a novel approximation to the Gittins index. Third, we take a step towards data efficient learning of high-dimensional control using Bayesian neural networks (BNN). Experimentally we show although filtering mitigates adverse effects of observation noise, much greater performance is achieved when optimising controllers with evaluations faithful to reality; by simulating closed-loop filtered control if executing closed-loop filtered control. Thus, controllers are optimised w.r.t. how they are used, outperforming filters applied to systems optimised by unfiltered simulations. We show directed exploration improves data efficiency. Lastly, we show BNN dynamics models are almost as data efficient as Gaussian process models. Results show data efficient learning of high-dimensional control is possible as BNNs scale to high-dimensional state inputs.","tags":null,"title":"Bayesian Learning for Data-Efficient Control","type":"publication"},{"authors":["Yarin Gal","Rowan McAllister","Carl Rasmussen"],"categories":null,"content":"","date":1466294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466294400,"objectID":"c7ee01b13488d6f9297c7dd9623af77d","permalink":"https://rowanmcallister.github.io/publication/dpilco/","publishdate":"2016-06-19T00:00:00Z","relpermalink":"/publication/dpilco/","section":"publication","summary":"Model-based reinforcement learning (RL) allows an agent to discover good policies with a small number of trials by generalising observed transitions. Data efficiency can be further improved with a probabilistic model of the agent’s ignorance about the world, allowing it to choose actions under uncertainty. Bayesian modelling offers tools for this task, with PILCO being a prominent example, achieving state-of-theart data efficiency on low dimensional RL benchmarks. But PILCO relies on Gaussian processes (GPs), which prohibits its applicability to problems that require a larger number of trials to be solved. Further, PILCO does not consider temporal correlation in model uncertainty between successive state transitions, which results in PILCO underestimating state uncertainty at future time steps. In this paper we extend PILCO’s framework to use Bayesian deep dynamics models with approximate variational inference, allowing PILCO to scale linearly with number of trials and observation space dimensionality. Using particle methods we sample dynamics function realisations, and obtain lower cumulative cost than PILCO. We give insights into the modelling assumptions made in PILCO, and show that moment matching is a crucial simplifying assumption made by the model. Our implementation can leverage GPU architectures, offering faster running time than PILCO, and will allow structured observation spaces to be modelled (images or higher dimensional inputs) in the future.","tags":null,"title":"Improving PILCO with Bayesian Neural Network Dynamics Models","type":"publication"},{"authors":["Thierry Peynot","Angela Lui","Rowan McAllister","Robert Fitch","Salah Sukkarieh"],"categories":null,"content":"","date":1410220800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1410220800,"objectID":"30c5e97300aab389a811a9150e0d5bba","permalink":"https://rowanmcallister.github.io/publication/mawson-jfr/","publishdate":"2014-09-09T00:00:00Z","relpermalink":"/publication/mawson-jfr/","section":"publication","summary":"Motion planning for planetary rovers must consider control uncertainty in order to maintain the safety of the platform during navigation. Modeling such control uncertainty is difficult due to the complex interaction between the platform and its environment. In this paper, we propose a motion-planning approach whereby the outcome of control actions is learned from experience and represented statistically using a Gaussian process regression model. This mobility prediction model is trained using sample executions of motion primitives on representative terrain, and it predicts the future outcome of control actions on similar terrain. Using Gaussian process regression allows us to exploit its inherent measure of prediction uncertainty in planning. We integrate mobility prediction into a Markov decision process framework and use dynamic programming to construct a control policy for navigation to a goal region in a terrain map built using an onboard depth sensor. We consider both rigid terrain, consisting of uneven ground, small rocks, and nontraversable rocks, and also deformable terrain. We introduce two methods for training the mobility prediction model from either proprioceptive or exteroceptive observations, and we report results from nearly 300 experimental trials using a planetary rover platform in a Mars-analogue environment. Our results validate the approach and demonstrate the value of planning under uncertainty for safe and reliable navigation.","tags":["Mawson the Robot"],"title":"Learned Stochastic Mobility Prediction for Planning with Control Uncertainty on Unstructured Terrain","type":"publication"},{"authors":["Rowan McAllister","Thierry Peynot","Robert Fitch","Salah Sukkarieh"],"categories":null,"content":"","date":1349568000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1349568000,"objectID":"b3fb5afc4488d633ceb3e93e4c0e7ac3","permalink":"https://rowanmcallister.github.io/publication/mawson-iros/","publishdate":"2012-10-07T00:00:00Z","relpermalink":"/publication/mawson-iros/","section":"publication","summary":"Motion planning for planetary rovers must consider control uncertainty in order to maintain the safety of the platform during navigation. Modelling such control uncertainty is difficult due to the complex interaction between the platform and its environment. In this paper, we propose a motion planning approach whereby the outcome of control actions is learned from experience and represented statistically using a Gaussian process regression model. This model is used to construct a control policy for navigation to a goal region in a terrain map built using an on-board RGB-D camera. The terrain includes flat ground, small rocks, and non-traversable rocks. We report the results of 200 simulated and 35 experimental trials that validate the approach and demonstrate the value of considering control uncertainty in maintaining platform safety.","tags":["Mawson the Robot"],"title":"Motion Planning and Stochastic Control with Experimental Validation on a Planetary Rover","type":"publication"},{"authors":["Rowan McAllister"],"categories":null,"content":"","date":1343779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1343779200,"objectID":"656dc45078960742f3bf0226a72e2834","permalink":"https://rowanmcallister.github.io/publication/mawson-masters/","publishdate":"2012-08-01T00:00:00Z","relpermalink":"/publication/mawson-masters/","section":"publication","summary":"Planetary rovers are required to safely navigate across unstructured and hazardous terrain with increasing levels of autonomy. Autonomy is necessary to react to impending danger because remote control has been proved challenging and dangerous for planetary rovers due to the large communication delays. Safety is especially a concern in space applications where a robot is unaccompanied throughout its entire mission. Unstructured terrain poses several types of hazards to a robot such as getting stuck, toppling or scraping against rocks. In a dense collection of localised hazards, platform safety is very sensitive to deviations from an intended path. To maintain the safety of the platform during navigation, planetary rovers must consider control uncertainty during motion planning. Thus, not only does the system need to make predictions of action outcomes, it also needs to estimate the accuracy of these predictions. The aim of this research is to provide planetary rovers with the ability to plan motions to goal regions that optimise the safety of the platform by including information about the accuracy of its controls. Modelling such control uncertainty is diffcult due to the complex interaction between the platform and its environment. In this thesis, we propose an approach to learn the outcome of control actions from experience, represented statistically using a Gaussian Process regression model. This model is incorporated explicitly in the planning process using dynamic programming to construct a control policy for navigation to a goal region. Motion planning strategies are considered that take into account different types of uncertainties, including uncertainty in distance, heading and yaw of the platform, across various motion primitives. The approach is implemented on a holonomic rover with six wheels and a Rocker-bogie frame and tested on a Mars analogue terrain that includes flat ground, small rocks, and non-traversable rocks. Planning is computed over a terrain map built using an on-board RGB-D camera. We report the results of 200 simulated and 95 experimental trials that validate the approach. These results demonstrate that explicitly incorporating knowledge of control uncertainty into the motion planning process increases platform safety by decreasing the likelihood of the rover getting stuck and reducing the cost accumulated over the executed path. Accounting for heading uncertainty resulted in the most significant increase in platform safety.","tags":["Mawson the Robot"],"title":"Motion Planning and Stochastic Control with Experimental Validation on a Planetary Rover","type":"publication"},{"authors":["Rowan McAllister"],"categories":null,"content":"","date":1257379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1257379200,"objectID":"e9c4264ea1e976ea01194cfffc8c9354","permalink":"https://rowanmcallister.github.io/publication/undergrad/","publishdate":"2009-11-05T00:00:00Z","relpermalink":"/publication/undergrad/","section":"publication","summary":"Self-Reconfiguring Robots (SRR) are composed of many modules that have the ability to autonomously attach and detach, enabling adaptation to a variety of tasks in unknown surroundings. In order to change shape into a particular form, an SRR must plan a sequence of module movements. This reconfiguration problem is challenging because of the many mechanical degrees of freedom and the resultant large number of possible SRR configurations which contribute to a vast and high-dimensional search space. To support the operation of an SRR in a practical environment, reconfiguration planners must satisfy a number of properties including decentralized computation, parallel motion of modules, real-time execution and planning in the native kinematic space of the module mechanism. The ultimate solution would be a general planner that solves reconfigurations of arbitrary module designs in this way. This thesis has taken a step in this direction of generality by developing a reconfiguration planner of the 3R module that is easily instantiable to other module types. The planner is scalable and decentralized, and considers the native kinematics of a module. It demonstrates the ability to coordinate the parallel motion of modules and executes in real-time. The thesis presents both centralized and decentralized implementations of the algorithm along with performance evaluation for several reconfiguration examples, as well as an analysis of achievable SRR reconfigurations.","tags":["Modular Robots"],"title":"Autonomous Reconfiguration Planning in Modular Robots","type":"publication"}]