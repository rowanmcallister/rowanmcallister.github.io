[{"authors":["admin"],"categories":null,"content":"I am a machine learning manager at the Toyota Research Institute in California focusing on autonomous vehicle planning. Previously, I was a PhD student at the Machine Learning Group with Carl Rasmussen and a postdoc in the Robotic AI and Learning Lab with Sergey Levine, working on probabilistic modeling toward data-efficient reinforcement learning and autonomous vehicle planning.\n","date":1653436800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1653436800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://rowanmcallister.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a machine learning manager at the Toyota Research Institute in California focusing on autonomous vehicle planning. Previously, I was a PhD student at the Machine Learning Group with Carl Rasmussen and a postdoc in the Robotic AI and Learning Lab with Sergey Levine, working on probabilistic modeling toward data-efficient reinforcement learning and autonomous vehicle planning.","tags":null,"title":"Rowan McAllister","type":"authors"},{"authors":["Rowan McAllister","Blake Wulfe","Jean Mercat","Logan Ellis","Sergey Levine","Adrien Gaidon","Sergey Levine"],"categories":null,"content":"","date":1653436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653436800,"objectID":"825e895d99f46fb5298c8e8c1937ad36","permalink":"https://rowanmcallister.github.io/publication/capo/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/capo/","section":"publication","summary":"Autonomous vehicle software is typically structured as a modular pipeline of individual components (e.g., perception, prediction, and planning) to help separate concerns into interpretable sub-tasks. Even when end-to-end training is possible, each module has its own set of objectives used for safety assurance, sample efficiency, regularization, or interpretability. However, intermediate objectives do not always align with overall system performance. For example, optimizing the likelihood of a trajectory prediction module might focus more on easy-to-predict agents than safety-critical or rare behaviors (e.g., jaywalking). In this paper, we present control-aware prediction objectives (CAPOs), to evaluate the downstream effect of predictions on control without requiring the planner be differentiable. We propose two types of importance weights that weight the predictive likelihood, one using an attention model between agents, and another based on control variation when exchanging predicted trajectories for ground truth trajectories. Experimentally, we show our objectives improve overall system performance in suburban driving scenarios using the CARLA simulator.","tags":["Autonomous Vehicles"],"title":"Control-Aware Prediction Objectives for Autonomous Driving","type":"publication"},{"authors":["Rowan McAllister"],"categories":[],"content":" This post is intended as a guide on how to organize an academic workshop at conferences in the fields of machine learning, robotics, and computer vision. Special thanks to Xinshuo Weng, Erin Grant, Andrea Bajcsy, Thomas Gilbert, Roberto Calandra, Yarin Gal, Pieter Abbeel, Wei-Lun (Harry) Chao for their helpful input, editing, and feedback. All opinions are my own, not necessarily shared.\nGetting Started Purpose Why organize a workshop? Workshops help foster a research community that you care about. Beyond sharing ideas, workshops help communities identify the open questions worth solving and build a sense of what to work on next through group discussions like panels. Such \u0026ldquo;collaborative work\u0026rdquo; and collective brainstorming can help a community make progress towards organizing open questions into short-to-long term topics that are likely impactful and tractable given the field\u0026rsquo;s current state. As an organizer, you can also influence the community\u0026rsquo;s attention towards underappreciated ideas and lesser-known researchers that you think deserve more attention, based on the workshop topic you choose and the keynote speakers you invite.\nA workshop is also a good networking opportunity. For attendees, your venue allows people with similar interests to mingle, share ideas, and socialize. If your workshop is interdisciplinary, it can facilitate cross-pollination of ideas between fields, and if your workshop topic centers on an application, it can build bridges between academia and industry. As an organizer, you’ll meet speakers and authors on friendly terms, having just provided them an additional opportunity to showcase their work. In addition, you’ll get to work closely with organizers from different labs and companies, including researchers from competing companies that you may not have been able to work with otherwise and get to know. Overall, it can be fun and rewarding.\nTopic The first thing to decide when organizing a workshop is: What should your workshop be about? Choose a workshop topic that you are passionate about, that you think is important, and that is timely for the community that you are trying to reach. This often means choosing a novel topic that is not yet mainstream but it is likely to play an important role in the next couple of years. Conversely, choosing a topic that was already explored in past workshops (e.g., a series of workshops on a single topic) requires some thought about how to keep the topic fresh and different from prior years. If you want to continue a topic from prior years, email the organizers from the previous event to see if they are planning to organize the workshop again. No one owns a topic, but it’s easier to collaborate on one proposal than to write competing proposals.\nThe next step is to select the scope. The right scope is a balance between being too broad (too vague to rally people around, or bearing little difference from the conference) and too niche (can’t attract crowds large enough to warrant the venue or have meaningful discussion). I recommend focusing on a family of methods (e.g., offline reinforcement learning or invertible models) or an application (e.g., CV for medicine or ML for climate), but not both (too specific). For example, see past workshop topics at ICRA, RSS, CVPR, NeurIPS.\nOrganizers How to start organizing a workshop? You shouldn’t do it alone since it’s too much work, and being a community event it’s good to have multiple perspectives, so your first step is to recruit co-organizers. Choose your co-organizers carefully, from a range of institutions, subfields, and backgrounds. It works best when organizers have assigned roles, with each organizer taking ownership over some aspect of the workshop (e.g., one of the headers in this document). Assigning responsibilities helps track all the TODOs so none are forgotten in addition to sharing the workload. For example, one co-organizer could be responsible for managing the workshop’s email account and website (easy), another conducting outreach (easy), and another to maintain contact with all your other invited speakers to ensure they are on track to submit the required materials (harder), and another to recruit a program committee and organize reviewing (harder). Discuss and establish roles as soon as possible, ideally before you submit your proposal. Ensure each organizer understands and feels comfortable about their roles too. This will help prevent a few organizers from eventually taking all the responsibilities.\nA critical role is the “lead organizer” to monitor everything at a high level. They can make executive decisions if needed (e.g., amid slow deliberations over email) and delegate incoming or scheduled tasks to ensure nothing falls through the cracks and everything happens on time. Lead organizers receive the most work and stress, being the one ultimately responsible for the workshop. So if you are the lead organizer, it is in your best interest to select co-organizers who you know you can rely on and who will actively and enthusiastically help out. Some workshops designate a “liaison role” to communicate with the conference’s workshop chairs (who oversee venue logistics for all workshops), but since the lead organizer needs to know about all these emails anyway, I find it simpler and faster for the lead organizer to be the liaison themselves.\nThe ideal organizational team is a mix between newer to more experienced organizers. Newer organizers can bring fresh suggestions of formats, themes, and speakers; while more experienced organizers can offer guidance on what works well and have larger networks to reach certain hard-to-reach speakers, recruit a program committee (aka reviewers), or seek other help. Workshop chairs who judge your proposal will be looking for both: newer organizers (less likely to rehash old ideas) while also assessing the organizational experience of the team (so they know you can do it). Some believe adding very senior-career organizers can help chances of workshop approval since their reputation can “legitimize” a workshop. I don’t know how true this is, possibly at IEEE conferences, but don’t count on them to actively help with the low-level details of the workshop. Bringing in new organizers is also a nice way to boost their experiences, resumes, and community presence (especially for more junior students) which is part of your positive impact on the community. But not too many, and you don’t want a team of extremes: half brand-new and half too-senior-to-meaningfully-help. You want a high-entropy distribution over experience, including organizers in-between who know what they are doing and actively helping, guiding, and monitoring. In addition to experiential diversity, geographic and demographic diversity will help you reach more speakers, create a workshop for everyone, and increase chances of your workshop’s approval.\nUnreliable organizers: Occasionally some organizers disappear after you’ve submitted your proposal. It’s not common, but it happens, more likely with a large number of organizers. NeurIPS considers more than 6 to be “too many organizers”, and I agree, unless you’re additionally organizing venues, audiovisuals, or competitions. Too many organizers can lead to a diffusion of responsibility. So select 4–6 organizers, who genuinely care about the workshop’s success, and clearly establish everyone’s responsibility in your proposal (including “advisors”).\nBottom line: Your main priority is finding a reliable, core group of people to help you with either (1) the substantial amount of work ahead or (2) offer reliable guidance, suggestions, and respond to all your questions and help out if you get stuck (e.g. with technical complexities using submission portals etc). So whatever their experience, select co-organizers who really do care about the workshop’s vision and success. If organizers are invested, your workshop will run smoother as organizers will likely think proactively about potential issues before they happen, rather than reacting to issues after they occur. And, if you really want to include any too-senior-to-do-any-work types for reputation points, consider listing them under “advisory committee” on your website, so those in the “organizing committee” don’t feel their recognition is unfairly diluted given their larger workload.\nStart early: To form a reliable organizational team, start 4–6 weeks before the proposal is due. This enables you to gradually enlarge your team: for example, after forming a team of three co-organizers, you may discuss with the team and reach out to a fourth person. Avoid last-minute invitations (e.g., a few days before the proposal due date) for two reasons. First, the newly invited person will not have enough time to provide comments on the proposal and is too late for speaker selection. Second, you may not have enough time to discuss the expected workload with them. This is important if the invitee has no prior experience: they may underestimate the workload and later become less active in the team.\nMoving forwards: Once the organization team is decided, it’s helpful to then video-conference together to discuss initial plans for what the workshop should be about and the rough structure organizers want (e.g. competitions/panels or not). Once a mutual understanding is established, it’s then easier to collaborate asynchronously over email or Google Docs to draft a schedule, add speaker suggestions, and discuss via comments; prior to preparing the workshop proposal. For multiple shared resources like documents, spreadsheets, slides, and videos; keep it all organized in a shared Drive folder that all organizers have edit access to. After your proposal is accepted, additional meetings prior to events like sending speaker invitations and starting the reviewing phase can help co-organizers stay involved. Without such follow up meetings, the main organizer might get stuck doing most of the work, unbeknownst to the other organizers.\nProposal Conferences will announce a “call for workshops” analogous to a “call for papers” that you can submit to. The application process mainly involves submitting a 2–5 page PDF proposal on what your workshop would be about and justifying why it’s worthwhile. Depending on the conference, they might accept 30–60% of these applications. So, workshop applications are competitive and it’s worth paying attention to various selection criteria to maximize your chances of acceptance.\nSelection Criteria Read the “call for workshops” (example) carefully, and any additional “guidance” (example) they offer detailing workshop evaluation. Workshop chairs will review your proposal based on criteria close to these, so ensure you have convincing answers to each of the selection criteria that are clear to readers of your proposal. Don’t bury important points on page 5, but present them early, bold keywords if needed so they are impossible to miss. Give special attention to (1) why your topic matters, (2) diversity of speakers and organizers (3) why your workshop is different from previous years if in a series. Note: the second largest NeurIPS workshop BDL was rejected in 2020 after workshop chairs deemed it too similar to their 2019 version). Other reasons NeurIPS 2019 workshops were rejected are outlined here too.\nChecklist: regardless of the conference\u0026rsquo;s guidance, I still make sure to address these following points clearly and in the early pages of my proposals:\n Purpose: Frame your proposal around how your workshop would benefit many attendees at the conference, not only some pre-existing community you care about. The conference will ultimately be selecting proposals around what makes the conference a success and impactful and what benefits its attendees. So clearly outline why the conference would benefit from hosting this workshop and why conference attendees would benefit from attending it. Explain why this topic is important and timely for a subset of the conference community that warrants more focus, and that a workshop at this particular conference would be the ideal venue for them to meet like-minded individuals, make meaningful connections, including between academic and industry, and ultimately create a positive impact on the world (workshop impact means conference impact). Ask yourself: why can\u0026rsquo;t authors just submit their papers to the conference itself; why do they really need to get together as a separate workshop? Diversity of speakers and organizers. When talking about diversity, try not to be vague or generic, otherwise it will seem like your commitment to diversity is in lip service only. I find it helpful to explicitly quantify how diverse our speakers and organizers are, using numbers on the breakdown of genders, race, geographic location, career stage, expertise, and who is new to this particular conference (bringing in outsiders can be a plus). For example, to illustrate career-stage diversity, I’ll say our speakers comprise: 1 student, 2 postdocs, 2 junior faculty, 1 senior faculty, 3 industry; and mention gender diversity like 4 women and 4 men; and geographic, e.g., 4 from the Americas, 2 from Asia, 2 Europe; and categorize their talks into subtopics to show how we’ll get a good coverage of topics throughout the day. You can summarize some of these differences into a table for clarity too (example). Chairs might not have realized this from just reading their names, and may have skimmed their bios, but it is now clear that speakers have diverse career stages, demographics, and locations. Fresh ideas: The proposal should highlight how the workshop will be different to prior related workshops, in terms of theme, format, and maybe some interesting ideas for poster sessions if the conference will be hybrid, etc. Your workshop shouldn’t just rehash the same topics discussed the previous year, but either be an emerging topic or somehow evolved from a prior year with a different focus even if the same topic. How will new organizers contribute to make the workshop different this year given their role? Engagement: Plans to advertise the call for papers, why the community will be excited by this topic, and why they will want to come together on this topic. Concretize this with numbers of related papers in related conferences or number of attendees at the previous or similar workshop. Also, describe how you plan to engage the attendees during the workshop, for example through poster sessions, breakout discussions, online polls at the start/end of the workshop about open questions, a debate, etc. Organizer experience: Highlight what workshops the proposed organizers have previously had experience with. It\u0026rsquo;s good to have organizers that have organized workshops before (to convince the workshop chairs that you know what you\u0026rsquo;re doing) but also that you have newer organizers who may have expertise in new and emerging directions. Solicit participation: Highlight how you’ll plan the call for papers (more important for IEEE conferences). I usually enumerate all the ways I plan to call for papers (see here).  A Fait Accompli Your workshop proposal must instill confidence that you know what you are doing. I find one way to do this is to craft the proposal in such a way that the workshop appears a fait accompli: everything is done and all speakers have confirmed and standing by, your workshop is clearly ready to go, all the workshop chairs need to do now is give permission to proceed! This means:\n Confirmed speakers: Invite all your speakers a month before the proposal is due, to get a list of confirmed speakers to list in your proposal. Emphasize that they are confirmed speakers in your proposal. Note: proposals to IEEE conferences like ICRA and IROS often require proof of each speaker’s conditional participation if the workshop is approved. You can either provide a commitment letter that you prepare and speakers sign, or (easier) a copy/screenshot of speakers’ emails confirming. You can ask the speakers if it’s OK to screenshot their email for this purpose. IEEE conferences also like seeing support or “endorsement”, such as a supporting letter by an RAS Technical Committee member. Schedule: Add a schedule to your proposal, formatted as a table, including specific times and event type. For talks, include the speaker’s name. For example, don’t write “Fourth event: Keynote Speaker 3”, write “10:00am, Keynote Speaker: Jane Doe”. You can always adjust later once speakers update their constraints closer to the date. Website: It helps if the workshop chairs can “see” the workshop they are reviewing. That means creating a website and displaying the URL prominently in your proposal. The website should look as polished as possible. Show the speakers (with images), schedule, organizers, and program committee if you have one, linking names to personal websites in case the workshop chairs want to check them out. Look at other previous accepted workshops for guidance on what a website “should” look like.  As an example, here is our ML4AD 2020 Workshop Proposal.\nSchedule Workshops often have a few types of events, like keynote talks and poster sessions, and possibly lightning talks, panels / debates, and competitions, spread out over a full day. This variety can make the workshop interesting and engaging to attendees. On the day, you’ll never stay perfectly on schedule (e.g. speakers might run over time), so ensure you have some coffee breaks interspersed throughout the day to prevent scheduling delays accumulating too much. Try to avoid last minute changes, but if you have no choice, make sure any schedule updates are reflected in both your workshop website and any conference apps/portals.\nPosters (2+ hours): Posters are the main event of your workshop! Attendees care about their ability to meet others, discuss their work, and discover new research ideas early on, all of which is possible with poster sessions. So schedule sufficient time for posters, they are not an afterthought. Organizers often underestimate how much poster time will work well. On paper, 1–1½ hours of posters might look like a lot, but it is not. Poster sessions take time to build momentum, as audience members find food, coffee, and check messages after talks wrap up (possibly running overtime too). So expect a “40 minute poster session” equals 30 minutes of posters. If you have 20+ posters, aim for 2+ hours of posters. Once people get into the poster sessions, time will fly, and you won’t enjoy breaking up all those interesting discussions prematurely because your schedule has some less-interactive event now starting. I usually schedule joint “posters and coffee” sessions, since people naturally look at posters during coffee—and vice versa—anyway. Consider having at least two poster sessions, e.g. morning and afternoon (in case one conflicts with other conference events etc, and especially for virtual events, to cover different time zones).\nLightning talks: One optional idea is to allow each author to briefly present their work in 1–3 minutes, which serves as an advertisement to attendees to visit their poster during the poster session, assuming you do not have too many authors. This can be more helpful for virtual conferences, since they lack the same serendipity that real poster sessions offer in discovering new works. For smooth transitions between many brief talks, you can request pre-recorded videos for virtual conferences, or use a shared slide deck for in-person presentations.\nKeynotes (4-8 speakers): Scheduling 4–8 speakers is a good bet for full day workshops: you can easily allocate 30–45 minutes to each speaker (for talk + questions) this way and still have time for other events like posters and panels. Schedule each speaker at least 10 minutes for questions so your audience can interact and engage with speakers properly. Interaction is what workshops are all about! Keep in mind speakers often go overtime too, so adequate question time helps act as a buffer.\nDate: sometimes conferences have multiple dates they run workshops on, and workshop chairs will ask for your preferred date. Now that many workshops are going back to a hybrid format with some people physically attending, try to avoid the last day of the conference, since some of those physically attending will leave early and you may have additional speaker scheduling constraints if they fly out that day.\nTimezone alignment: Hybrid or fully-virtual workshops pose extra benefits and challenges. More people are able to attend the event without traveling to the physical conference location, thus increasing your audience, but it can be difficult to satisfy attendee, speaker, and organizer constraints when scheduling events across many time zones. To make your workshop accessible to people from various time zones, consider breaking the schedule into multiple parts, for example by broad geographic location (e.g., Asia, Europe, Americas; example planner), so that most attendees will have at least one workshop component at a convenient time. This is also where having selected a diverse set of organizers and speakers will pay off, since you can more easily ensure coverage of each workshop segment. You can even add Google Analytics to your website to see the geographical distribution of interest in your workshop and select times friendly to where the interest is.\nTimeline Your workshop was approved, congrats! 🎉 Now the real work begins. Your next job is to set up a meeting with your conference co-organizers. The goal of the meeting is to finalize the role assignments and the important dates of your workshop’s timeline: the call for papers date → submission due date → reviewing dates → decision+notification date → camera-ready due date → workshop date. I recommend all deadlines be scheduled at 23:59 (11:59PM) “Anywhere on Earth” timezone (AoE) to reduce confusion. After the meeting, you may also want to send an email to the confirmed speakers, confirming with them that the workshop is accepted.\nCall for papers: Send out a call for papers as soon as you’ve updated your website with your workshop’s timeline, paper format, and submission portal. Post about your workshop sometime Tuesday–Thursday so more people notice it, and then follow up periodically with a few more online reminder posts in the lead up to the deadline (in case people miss the first one).\nSubmission date: Set the submission deadline as late as possible to attract more papers and more recent papers, while still giving enough time for reviewing and then the authors to organize visas after notification. Try to set the deadline after the conference’s notification date to attract conference rejections too (which hopefully improve when submitted to your workshop). Many authors of rejected conference papers still want to attend the conference regardless and submitting to your workshop might still be reason enough for their lab to fund them to go. Avoid setting deadlines that fall on a weekend or common holiday.\nNotification date: If a physical workshop, make sure to set your notification date so authors have enough time to get visas. Some conferences set a “latest date” to notify authors in their call for workshops. Some workshops have “early bird” prices: consider setting workshop decision/notification date to be before the early bird prices stop so authors have a chance to save money. Soon after the notification date (or concurrently) you’ll want to decide on contributed / spotlight talks (if any) so authors have time to prepare.\nSecond submission date? Since selecting a single submission date is an unfortunate compromise between having adequate visa processing time and attracting more papers, some workshops have two rounds of submissions instead. This could be planned, with both submission deadlines advertised together in advance, or an unplanned “deadline extension” if your workshop doesn\u0026rsquo;t receive many submissions initially. A second deadline helps attract more papers. When NeurIPS began requiring early notification dates of all workshops in 2019, their biggest workshop (Deep RL) started including a second call for “late-breaking” papers to attract arXiv submissions popping up after the first deadline. You can decide if late breaking papers should be held to the same standard as the first round of submissions, or if they should represent more exploratory work, maybe less mature with only partially validated preliminary results but nevertheless cutting edge, thought provoking, and novel.\nOutreach: Schedule a set of announcements and reminders about your workshop to social media (Twitter, LinkedIn, Facebook, Instagram) to remind people about your workshop beyond a one-and-done call for papers that people might forget about. Consider assigning the role of “social media czar” to one of your co-organizers so you know it will get done reliably.\n Speakers Speaker Selection How Many? A common mistake new organizers make is inviting too many speakers. Ten speakers might impress those reviewing your proposal, but isn’t necessarily what’s best for your audience. For a full day workshop, 4–8 speakers is good, just ensure you allocate sufficient time for posters. Poster sessions should not be squeezed into your schedule between too many speakers as an afterthought since poster sessions are the main event of your workshop!\nBig Names: Inviting big name speakers is a trade off. Big names can draw big crowds with their brand. However, big names with big responsibilities are more likely to cancel at the last minute when something else more important comes up, or simply repeat a previous talk if they are busy. Exceptions exist of course, some folk are very reliable. So consider only 1–2 big names if you want a larger crowd without taking on excessive risk, and consider scheduling them as “bookends” in your schedule, i.e. last morning speaker and last afternoon speaker, to maximize attendance throughout the day, while minimizing risk: if they cancel, you just finish that session early, avoiding a major schedule disruption. Most invites should ideally be to high-quality, engaged, and “undiscovered” junior researchers. For instance, junior faculty, postdocs, even senior-PhDs; as they are more reliable, more responsive, closer to the tech, and your workshop can have a greater impact on their career while they try to establish themselves, their students, and their labs within the academic community (this is part of your positive impact on the evolving community). Note: some call for workshops list selection criteria including “quality of proposed invited speakers” (NeurIPS language). “Quality” does not mean popular, it means inviting those who can present interesting and impactful scientific discoveries. So consider writing a short summary on each speaker in your workshop proposal giving links to their website, works, and any past talks to support your selection. On the flip side, try to warn brand-new faculty against rehashing their recent job talks since we want more depth, less breath, in our workshop talks.\nFresh Faces: To encourage fresh ideas and fresh faces, I follow a simple rule: no speaker gets reinvited back to the same workshop series. Or if this is a new workshop topic, check who spoke at related workshops in other conferences recently, and select different speakers. This doesn’t always happen, since other organizers have a say too, but I think it’s a nice rule to challenge yourself to seek out speakers you weren’t previously aware of and increase diversity of ideas. Having organizers from different institutions and different nations will help generate a wider set of initial speaker suggestions initially too.\nDiversity: Diversity is important to the community and conference organizers now take this more seriously. Beyond better representing the community and promoting inclusiveness, diversity is a critical selection criteria for workshops, meaning diversity in gender, race, geographic location, subspeciality, and career stage. So think about the set of speakers you want your workshop to have, not just each speaker individually. Optimizing for a set of speakers might require staggered invites as some will agree to speak and some will disagree, so give yourself enough time (4–6 weeks) before the workshop-submission deadline to send out multiple invitation waves to get your confirmed speaker set you can add into your proposal.\nSpeaker Invitations Let newer organizers invite speakers: It’s nice to let newer organizers invite speakers (cc’ing all other organizers) to have a chance at making new connections and build rapport with speakers who might be further along career-wise. If other organizers know the speaker well, they can always chime in afterwards with “hope you can make it Steph!” etc.\nAdd a “respond by” date: When inviting speakers, some don’t respond. If you assume that’s a “no”, and invite someone else as replacement, will the original invitee respond “yes” later? To avoid any awkward situations, add a “please respond by {day, date}” when you send invitations, leaving sufficient time before the proposal is due to invite other speakers.\nIndustry speakers: If you invite industry speakers to your academic workshop, clarify that this is not a marketing pitch and ask that they only accept if they are able to speak about recent technical content. Sounds obvious, but not to everyone. Your audience won’t appreciate a surprise PR show with flashy videos and vague statements about futuristic technology, they want technical detail. Note that many industry speakers require getting their talk approved. So it’s good to ask early, so they can start preparing and getting their talk approved in time for the workshop.\nKeeping in touch: There may be many months between the initial invite and the workshop event. It can be good to occasionally keep them updated with relevant developments (like when your workshop is selected, and when the conference selects which date your workshop will be). Writing update emails to speakers can be a good excuse to remind them of your call for papers (if it’s been called yet) and invite them or their lab to submit works too. Since they are now a part of your workshop, they are more likely to submit papers too.\nPanel You might consider adding a 30–60 minute panel discussion event to your workshop, where you invite keynote speakers back to discuss (or debate) the current state of research. A good panel discussion is both informative and entertaining. To be informative: you want to help people to understand the technical research landscape better, especially newer researchers trying to work out a direction. So consider posing the following questions to the panel:\n When should we use method A over B? What are better ways of thinking about current problems on this topic? What problems are important but currently underexplored (i.e. what research directions might new students and researchers consider pursuing now)?  It can also be fun to poll the community about which discussion topics they would like to see. For example, you can post a poll on twitter about different debate topics.\nPanels can also be entertaining. An experienced moderator is important here, who understands the right questions to ask, to encourage a healthy amount of opinion, jest, and rivalry among competing popular ideas in the field to keep the panelists and audience engaged. Make sure to include the moderator as one of the established organizer roles. A nice format the Bayesian deep learning workshop employed was inviting Yann LeCun as the “Anti-Bayesian” voice to play devil\u0026rsquo;s advocate, disrupting the Bayesians’ echo chamber and (respectfully) challenging other panelists on their fundamental assumptions about Bayesianism and suggesting alternatives to their favorite methods. It makes for a fun and lively debate with points made both philosophical and pragmatic.\nIndustry panelists: Not every industry speaker can speak as freely on certain topics as academic speakers can. Often industry speakers require their internal legal or PR teams to approve their talks and slides in advance, but cannot pre-approve unknown panel questions. So be wary of having too many industry speakers on your panel if your workshop topic centers on an application, or at least ask them what their constraints are in advance.\n Papers Call for Papers There are multiple ways to call on the community to submit papers to your workshop:\n Social (Twitter, LinkedIn, Facebook, Instagram)  Images go great with posts, they are more noticeable. You could use a graphic that represents your workshop’s theme or an image of your confirmed speakers. Tag your speakers and co-organizers, they often want to share and promote it. Tag the host conference in your call for papers post too, using their hashtag or handle (e.g. ICRA’s are #ICRA2022 and @ieee_ras_icra). Conferences often like sharing these to all the conference attendees to promote your workshop.  Mailing lists:  ML news Robotics Worldwide Black in AI (see \u0026ldquo;share opportunities\u0026rdquo;) Women in ML Queer in AI Europe  Institutions: Advertising within each organizer’s lab, university, or company. Individuals you know would be interested.  Paper Format I often copy style files that the conference papers use, editing slightly with a footer that says it’s our workshop. Example style file, tex template, and resultant pdf, stating: “we welcome papers up to 8 pages (max) not including references or appendix, as a single PDF”, and set CMT to only allow one PDF upload per submission (easier for everyone if appendix isn’t separate).\nExtended abstracts: Whatever format you choose, consider allowing 4-page submissions (“extended abstracts”). This is because the dual/double submission policy at some computer vision conferences include peer-reviewed workshop papers exceeding 4 pages (e.g. see CVPR or ECCV rules). Extended abstracts allow authors to submit preliminary results to your workshop before a conference submission, which are the types of papers you want: exciting up-and-coming ideas that might not even be on arxiv yet.\nSubmissions If you expect 15+ submissions, then use a submission portal to keep track of everything. Options include:\n Conference Management Toolkit (see how-to guide) OpenReview EasyChair  Single blind or double blind? Single blind means camera-ready submission can be optional or non-existent if you want to create less work for authors. Double blind is probably more fair.\nArchival or non-archival? An archival or formally published workshop proceedings often precludes authors from submitting an extended version of the same work to a more formal venue such as a conference or journal. Some authors therefore ask about this so I usually specify on the website that “no submission will be indexed nor have archival proceedings”. In some situations, you may invite submission for both proceedings and non-proceedings. For example, full-length papers go to proceedings, extended abstracts go to non-proceedings.\nReviewing Reviewing submitted papers is optional. I usually provide reviews since feedback is helpful to authors, but not all workshops do. Some workshops like Deep RL are stopping the practice, and were already lightweight in previous years, largely checking for fit, there being actual content, and a selection of stronger papers for longer presentations. Such lightweight reviewing is more lenient, only rejecting poor or off-topic submissions, corresponding to an acceptance rate of about 90%. Detailed reviewing is more valuable to authors but places more burden on reviewers and can be less practical for larger workshops to provide reviews of consistent quality. If you wish to provide reviews, consider recruiting a program committee (aka reviewers) if you expect 15+ papers, otherwise the organizers can review the papers themselves.\nProgram Committee Be courteous: To invite people to help review, I find people are much more likely to respond to a personalized invite from a human rather than an automated email from your submission portal. So I usually reach out to people first to ask, and add them into a reviewing system like CMT once they accept and are expecting automated emails (I learned this from Pieter Abbeel, his individual invites to review for the Deep RL workshop were always so polite, I couldn’t say no!). When writing the email, I explain what I’m trying to do, and ask if they’d like to help review. They are busy, and this is “only” a workshop—not a conference—so promise only 1–3 reviews per reviewer (not more than 3) to keep it light, hopefully even fun. Keeping your committee happy means they’ll likely agree to help in future years too. You can also cc your co-organizers on the email to increase the recognition each reviewer receives, though if sending many reviewer invites then consider scheduling the emails to send at the same specific time (gmail can do this) to avoid distracting your co-organizers with a slow trickle of emails.\nHow many reviewers? Aim for 3 reviews per paper so that the majority of papers receive 2–3 reviews. That means inviting about as many reviewers as expected submissions. Tell them the reviewing dates, and follow up with 1–2 polite reminders as the due date approaches to those yet to review. If you do this, only a minority of submissions will receive 0–1 reviews. Many reviewers only do reviews close to the actual deadline, so if you schedule a couple days between the reviewing due date and the notification date, then you can give late reviewers a little more time if they ask for it.\nEmergency reviewers: In anticipation of some papers with 0–1 reviews, you can recruit some “emergency reviewers” in advance to be “on call” for the couple days prior to paper decisions, so that every paper can have at least 2 reviews by the time you made your acceptance/rejection decisions. And if all else fails, you review the papers with fewer than 2 reviews.\nSet expectations: You want to attract novel ideas that are scientifically interesting, on topic, and would create interesting discussions at the workshop. No need to be super critical of experimental results. Workshop papers are not conference papers so communicate to reviewers the criteria you’re looking for.\nReviewer questions: Reviewing shouldn’t be laborious for your program committee, so consider limiting the amount of long-answer fields. If you use CMT, you can customize the review form and add instructions to each field. You even can add the reviewing rubric to your website (example) to help answer submitters’ questions like “my paper contains ABC, but not XYZ, is this good enough for a submission?”. Consider including a score on reviewer confidence too.\nPaper matching: To match reviewers to papers, several tools are available. CMT has some options including random allocation, bidding, or using the Toronto Paper Matching System. Proper matching benefits are nice for authors and reviewers alike, especially if your topic is broad, or is an application (not method) of AI. For small workshops, bidding is probably overkill and takes time, so I manually match reviewers, by looking up their google scholar to classify their expertise in a spreadsheet, then classify the papers into clusters of subtopics on the same spreadsheet, then match like-to-like. Systems like CMT help avoid certain conflicts of interest by preventing you from accidentally matching an author and reviewer from the same institution.\nDecisions You can set the bar for acceptance however you like. Generally workshop decision thresholds are more lenient than conferences, resulting in the acceptance of 50–90% of submissions. I usually aim for three reviews per paper, and then make decisions based on the three reviewer ratings as follows:\n No accept ratings = reject 1\u0026frasl;3 accept ratings = investigate, read reviews thoroughly, organizer makes decision 2\u0026frasl;3 accept ratings = probably accept (check review that rated submission as reject) 3\u0026frasl;3 accept ratings = accept  Keep in mind some reviewers will review in “conference mode” as if these were conference papers, despite any expectations you set initially. Workshop papers are not conference papers (yet). You want to attract new, scientifically interesting ideas; even if the experimental results are premature, or don’t outperform SOTA, or even compare to it yet. So if the idea is on-topic and novel, consider dismissing any reject ratings that were based on lackluster experimental results and comparisons. And finally, if after all that, your count of accepted papers is lower than what you would like, you can do it all again with a second call for papers! Assuming you have time, that is. A second late round isn’t as useful for people who need time for visas, but it’ll increase the average recency of ideas at your workshop.\nNotification: Once you’ve decided which papers to accept and reject, then notify the authors by your promised notification date (which should be on your website). If papers were reviewed, make sure the reviews are viewable by the authors and remind them of the link so they can use the feedback to improve their camera-ready paper if accepted or some other venue if rejected.\nCompetitions You could consider integrating a competition or “challenge” into your workshop. There are two ways to do this:\n You can organize a competition yourself, which requires significant effort; but you retain control, gain recognition, and it can be a great way to launch some new benchmark or dataset you collected and wish to popularize and share. Common competition hosting platforms to use for algorithm evaluation are AIcrowd, CodaLab, EvalAI, and Kaggle. For examples, see the CVPR workshops such as Embodied AI. Alternatively, you can host a competition: invite some group who was hoping to run a related competition anyway and link it up with your workshop (what I do). Hosting should be mutually beneficial: competition organizers are often looking for venues (and sometimes legitimacy) to showcase their winners’ methods and you might be looking for more ways people can engage with your event. Some workshops host multiple competitions, but do consider if your audience really benefits from sitting through multiple competition explanations and award ceremonies. Your audience may prefer more talks or poster time instead.  Competitions are optional, common at computer vision workshops, but not expected. As an idea, in 2021, 54% of CVPR workshops either hosted or organized a competition, compared to 2% at NeurIPS (machine learning), and 0% at RSS (robotics). But that doesn’t mean you can’t incorporate a competition into your own ML or robotics workshop! Our ML4AD was the only NeurIPS workshop to host a competition but we found it rewarding to offer an additional way to engage with our workshop beyond papers and posters without requiring too much of our effort.\nSome conferences host competitions themselves (e.g., NeurIPS has a Call for Competitions). This does not mean that your workshop cannot host its own competition too, it’s a decision for the competition organizers. You won’t benefit much from collaborating with competitions that you are not hosting. If they are strongly on-topic and ask to talk or advertise at your workshop, you could give them 10–15 mins. If you host the competition, then your workshop benefits by bringing their crowd to your venue.\n Event Before the Workshop My checklist:\n Organizer presence: Check which organizers will attend the conferences (especially for physical ones) and stay the whole day. Ensure you have a minimum 2, ideally 3+. Prepare introductions for each speaker (their bio) for a warm introduction and smooth transitions between speakers. If you are unsure how to pronounce their name, ask them privately in advance. Obtain any consent+release forms (permission to record) from speakers supplied to you by any recording groups the conference uses (if any) like SlidesLive Print messages: To stay on schedule, help speakers avoid going overtime by bringing printed numbers that you can hold up to inform them how many minutes they have left Print schedules to pin up outside the room (so conference attendees walking outside can check when they might want to join) Spare stuff: Bring spare AV adaptors including Apple adaptors, since AV issues often arise on the day, and spares are useful. Also bring your laptop, USB drives; and spare tape and push pins for posters. Backup questions: Prepare “backup questions” for each keynote or spotlight speaker. When each speaker concludes their talk, you’ll invite questions from the audience, but sometimes they cannot think of anything to ask initially. In such cases, it’s great to have some questions pre-prepared. You can prepare by watching speakers\u0026rsquo; talks in advance if recorded, or reading their recent on-topic papers in advance.  Day of the Workshop It is likely that something will go wrong on the day of your workshop. Redundancy helps here. So have multiple organizers attending the workshop to deal with issues together. If virtual, one of you may lose internet connection, so have 2+ organizers online at any one time. If in-person, carry a spare laptop, AV adapters, and USB drive in case speakers have equipment issues. I Have a private schedule with the other organizers who will be the master of ceremonies (MC) when, and who will be watching on standby as “backup MC” ready to take over within a few seconds if something goes wrong (like the MC losing internet connection).\nMy checklist: arrive early and\n Reserve front seats for organizers + speakers. Greet the audio-visual staff to understand how everything will work, and explain any signals they’ll be gesturing toward you from the back when you’re up at the front talking during the day. There’s often AV issues on the day (a speaker doesn’t have the right adapter etc) and the AV team is critical here, so it’s good to get to know them, and thank them at your workshop’s conclusion.  Poster Sessions Attend your own workshop! Go chat to authors during the poster sessions and learn about their work. As an organizer, I recommend visiting posters that are not receiving as much attention. People are social, and crowds assume social proof that certain posters are better if people are there, creating unequal distributions of crowded posters and lonely posters. You can counteract this bias by visiting less populated posters, which will attract others.\nTalks Even if you prepared “backup questions” for each speaker in advance, for non-recorded talks, you’ll want at least one organizer (the MC) to be paying attention to each speaker during the event to think of backup questions live. If no one asks a question, then you can ask your backup question. Questions beget questions too, it will warm up the audience. This is especially helpful for virtual conferences. Virtual attendees often prefer typing questions too, so remind the audience they can type their questions into the Zoom chat or question field during the talk, so you can immediately begin the Q\u0026amp;A.\nAfter the Workshop After the event, there’s several options to consider\n Thank audio-visual staff: After the workshop, thank the audio-visual staff and consider adding their names to the website. They do much critical work behind the scenes. Get feedback, e.g., via a link on your website or email to improve next year (example). Dinner: You can organize a dinner for the speakers and (if you have them) sponsors. This is nice for the speakers, and a good opportunity to get to know them better. It can be a bit pricey to buy a dinner for ~10 people, but one of your sponsors may be happy to pay the bill. Have some time between the workshop end and dinner though, people often want a break, or want to chat to others after the workshop instead of rushing off to dinner straight away. Link recordings to the website. It’s nice for speakers to have public links to their talks and for others who couldn’t make your event to see. Summarize submissions: Some organizers like writing up a paper or blog or website to summarize the workshop, something easily spread on social media. E.g. a teaser figure and short description and links for each paper. This helps give further publicity to workshop papers and the workshop itself. One group actually wrote a blog summarizing my own workshop without me which was amusing. At ICCV we wrote a summary paper after the event. Most workshops do not bother with this, this is very optional.  Website You’ll need a webpage, to summarize the speakers, schedule, and how to submit papers.\nAccessibility: Not all websites are accessible to everyone. Google Sites is very easy to use, but not accessible in China. GitHub Pages is a little harder to use, but accessible in more countries.\n Custom URLs: Alternatively, you can provide custom URLs to make certain sites accessible. For example https://www.icra2022av.org (accessible in China) is actually this website under the hood https://sites.google.com/view/icra2022av/home (inaccessible in China), so we just communicate the .org domain version. You can do this by going to your google site, then settings→ custom domains → add → buy a domain (~$12 a year). Github or Google Sites?: An advantage of google sites: quick to create + upkeep, for anyone in team. Advantage of github: get more control on appearance and enable javascript. For example, people can get confused with timezones, so I always use the “Anywhere on Earth” (AoE) timezone, and include a javascript countdown so people know exactly when the deadline is, or linking to https://time.is/Anywhere_on_Earth. For github I used this template which looks like this initially, which I change to look like this. Or use GitHub Pages which is simpler, looks like this; or http://jekyllrb.com/ which looks like this. An advantage of github is you can upload and host papers directly on your website. Other: You could also host and manage websites using Amazon S3 buckets  Timezone help: To help virtual attendees avoid doing timezone conversations themselves, you can display the time of each section of the workshop in multiple major time zones on your website’s schedule, or use javascript to display event times in the attendee\u0026rsquo;s local timezone (example). You can also include an ICS-formatted calendar they can download too (example). Or assuming most people use Google Calendar, you can add one-click per-event invites. To do this, first add a new workshop calendar, then click calendar options →check “Make available to public” check true (details). Second, publish each event individually (click three dots for the Options menu, click “Publish event”, copy “Link to event”). Another alternative when using a github webpage, is to include a javascript “venue time” clock (local time at the venue) on the schedule.\nHelpful Tips / FAQ: If the workshop is physical, I often like to add suggestions where authors can print their posters near the venue (easier for one person googling this than 50). Some workshops even include poster templates.\nStreaming: Add a streaming link on your website if the conference allows it. For example, you can stream your Zoom meeting on YouTubeLive.\nVirtual Tools Streaming talks:\n SlidesLive (example) Twitch Zoom Pro YouTubeLive (note you must sign up for this at least 24 hours in advance: it takes 24 hours exactly between you requesting to stream with Youtube and it being enabled) Via Zoom (instructions) (example) Streaming talks without interaction  CrowdCast Facebook Live   Streaming questions with upvoting + moderation:\n Slido (how-to video) Pigeonhole Live (how-to videos) Mentimeter (how-to video) Zoom Q\u0026amp;A Slack RocketChat (used at ICLR)  Poster Sessions:\n Gather Town and Spatial Chat: 2D avatar environments  Join by browser, no installation required. Can upload poster images and link to videos (our example) Proximity-based audio+visuals to other avatars. Gather Town is very popular, many people are used to it now.  Mozilla Hubs, a 3D avatar environment (our example) (how-to videos)  Join by browser, no installation required. Posters should be imported as images, since pdf rendering is poor. Use a larger font size than you would for a regular poster. Importing videos renders well. Used at IEEE-VR conference  Microsoft Teams Hopin Per-author Zoom links (generally a bad idea, lots of lonely rooms) Or avoid poster sessions, change to short author talks, with general Q\u0026amp;A after.  Sponsorship Workshop sponsorship is optional. Sponsors can contribute cash, cloud credits, or hardware. Some workshops use sponsorship to fund best paper awards. Although, I doubt prize amounts not exceeding the cost of travel provide extra motivation to submit. Researchers are already motivated to submit papers and often best-papers are won by well-funded labs already. In my opinion, it\u0026rsquo;s better to fund student travel awards based on financial need, to enable more people to attend your workshop or at least reduce the financial stress of doing so (example application). It helps create a more equal research community and can increase physical attendance. However, workshop sponsorship is often more trouble than it’s worth, for reasons below. An exception is for competitions, which usually need prizes to generate initial interest.\nWarning: handling funds can be difficult. If you have multiple sponsors, then holding funds together can be difficult. You cannot hold funds in your own personal bank account. You can open a new bank account, or you can ask a trusted third party like a university to hold funds for you, for a fee (e.g. Toronto might if you’re a student/staff there, Berkeley won’t unless the event is on campus). The easiest option I find is to ask sponsors to transfer to recipients directly without involving third parties. When companies want to sponsor using their corporate credit card, use PayPal. PayPal is the easiest way to make international card transactions to individuals, and works in almost every country. If the sponsor requests an invoice for internal accounting purposes, I use one of these templates. If the sponsor is US based, they might request collecting W-9 or W-8BENE tax forms for US and non-US prize winners.\nEnsure fairness and consider sponsor constraints. Hardware companies might sponsor by handing you a GPU or laptop at the event to hand to winners on stage (be careful during Covid though, some US companies can only ship to certain regions: US/Canada/Europe/Africa, but not Asia, not even to Asia indirectly via your address, due to government sanctions). Whoever your sponsor is, be very clear about what their constraints are and what strings they want to attach in advance. For example, some sponsors stipulate the winner must be from a university. Also inquire if there are any nationalities they cannot transfer to (especially if they sponsor computer hardware as a prize), and if they cannot sponsor winners from certain nationalities that you’d expect at your workshop (e.g. because of sanctions or company policy), then drop the sponsor. Note: they may not exactly be upfront about these international constraints (bad PR), so you should inquire and verify. And they may be unwilling to say over email, but might tell you over video call. But if they ever tell you “check with us before you choose who to award the prize”, do not allow them to sponsor, as there is something that they are not telling you. You must ensure that your process of selecting prize winners is fair, and not conditional on unknown sponsor constraints.\nOther considerations. Sponsors might also request proof that prize recipients are in fact the competition or best-paper winners. Emails and websites might not count as proof, but event video recordings probably will. Find out in advance!\n Appendix Website JavaScript Submission Countdown \u0026lt;p\u0026gt;Submissions due: 1st January 2022 at 23:59 Anywhere on Earth: \u0026lt;span id=\u0026quot;countdown\u0026quot;\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;script\u0026gt; // Set the date we're counting down to var countDownDate = new Date(\u0026quot;Nov 19, 2021 23:59:59 UTC\u0026quot;).getTime(); // enter time here in AoE countDownDate = countDownDate + 1000 * 3600 * 12 // AoE = UTC - 12 // Update the count down every 1 second var x = setInterval(function() { // Get today's date and time var now = new Date().getTime(); // Find the distance between now and the count down date var distance = countDownDate - now; // Time calculations for days, hours, minutes and seconds var days = Math.floor(distance / (1000 * 60 * 60 * 24)); var hours = Math.floor((distance % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60)); var minutes = Math.floor((distance % (1000 * 60 * 60)) / (1000 * 60)); var seconds = Math.floor((distance % (1000 * 60)) / 1000); // Display the result in the element with id=\u0026quot;countdown\u0026quot; var countdown = days + \u0026quot;d \u0026quot; + hours + \u0026quot;h \u0026quot; + minutes + \u0026quot;m \u0026quot; + seconds + \u0026quot;s \u0026quot;; // If the countdown is finished, write some text if (distance \u0026lt; 0) { clearInterval(x); countdown = \u0026quot;Hurry, submissions closing soon!\u0026quot;; // optional message if countdown expired } document.getElementById(\u0026quot;countdown\u0026quot;).innerHTML = countdown }, 1000); \u0026lt;/script\u0026gt;  Display Live Venue Time in Schedule \u0026lt;p\u0026gt;All times are in Pacific Time. Current time is \u0026lt;span id=\u0026quot;pacifictime\u0026quot;\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;script\u0026gt; // Update the count down every 1 second var x = setInterval(function() { var d = new Date(); var n = d.toLocaleTimeString(\u0026quot;en-US\u0026quot;, {timeZone: \u0026quot;America/Los_Angeles\u0026quot;, hour: '2-digit', minute:'2-digit', hour12: false}) document.getElementById(\u0026quot;pacifictime\u0026quot;).innerHTML = n }, 1000); \u0026lt;/script\u0026gt;  Display Schedule Events in User\u0026rsquo;s Timezone \u0026lt;p\u0026gt; Keynote talk by Alice at \u0026lt;span id=\u0026quot;keynote1_time\u0026quot;\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt; Keynote talk by Bob at \u0026lt;span id=\u0026quot;keynote2_time\u0026quot;\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;script\u0026gt; // specify schedule in UTC const keynote1_utc = new Date('May 10, 2022 10:00 GMT+00:00'); const keynote2_utc = new Date('May 10, 2022 11:00 GMT+00:00'); // displays in the user's local timezone document.getElementById(\u0026quot;keynote1_time\u0026quot;).innerHTML = keynote1_utc; document.getElementById(\u0026quot;keynote2_time\u0026quot;).innerHTML = keynote2_utc; \u0026lt;/script\u0026gt;  Sources This advice comes from colleagues and my own experience organizing the following workshops:\n ICLR 2019 Task-Agnostic Reinforcement Learning NeurIPS 2019 Machine Learning for Autonomous Driving RSS 2020 Interaction and Decision-Making in Autonomous-Driving ICML 2020 AI for Autonomous Driving ECCV 2020 Perception for Autonomous Driving NeurIPS 2020 Machine Learning for Autonomous Driving ICCV 2021 Multi-Agent Interaction and Relational Reasoning ICCV 2021 Autonomous Vehicle Vision NeurIPS 2021 Machine Learning for Autonomous Driving ICRA 2022 Fresh Perspectives on the Future of Autonomous Driving  You can download a printer-friendly version of this post here.\n","date":1651708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651708800,"objectID":"76ca418c1be6912035b33bf0568a7423","permalink":"https://rowanmcallister.github.io/post/workshops/","publishdate":"2022-05-05T00:00:00Z","relpermalink":"/post/workshops/","section":"post","summary":"Advice on workshop organization at academic conferences","tags":null,"title":"Workshop organization guide","type":"post"},{"authors":["Blake Wulfe","Ashwin Balakrishna","Logan Ellis","Jean Mercat","Rowan McAllister","Adrien Gaidon"],"categories":null,"content":"","date":1643068800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643068800,"objectID":"28472646a894a9e3df7e2e4fa70840c4","permalink":"https://rowanmcallister.github.io/publication/dard/","publishdate":"2022-01-25T00:00:00Z","relpermalink":"/publication/dard/","section":"publication","summary":"The ability to learn reward functions plays an important role in enabling the deployment of intelligent agents in the real world. However, comparing reward functions, for example as a means of evaluating reward learning methods, presents a challenge. Reward functions are typically compared by considering the behavior of optimized policies, but this approach conflates deficiencies in the reward function with those of the policy search algorithm used to optimize it. To address this challenge, Gleave et al. (2020) propose the Equivalent-Policy Invariant Comparison (EPIC) distance. EPIC avoids policy optimization, but in doing so requires computing reward values at transitions that may be impossible under the system dynamics. This is problematic for learned reward functions because it entails evaluating them outside of their training distribution, resulting in inaccurate reward values that we show can render EPIC ineffective at comparing rewards. To address this problem, we propose the Dynamics-Aware Reward Distance (DARD), a new reward pseudometric. DARD uses an approximate transition model of the environment to transform reward functions into a form that allows for comparisons that are invariant to reward shaping while only evaluating reward functions on transitions close to their training distribution. Experiments in simulated physical domains demonstrate that DARD enables reliable reward comparisons without policy optimization and is significantly more predictive than baseline methods of downstream policy performance when dealing with learned reward functions.","tags":["Reinforcement Learning"],"title":"Dynamics-Aware Comparison of Learned Reward Functions","type":"publication"},{"authors":["Rowan McAllister"],"categories":[],"content":" Choosing the right research problem is difficult. Both Vladlen Koltun and Richard Hamming offer some great advice on selecting which research problems to work on and how. This post is simply a summary of Richard\u0026rsquo;s talk \u0026ldquo;You and Your Research\u0026rdquo;, and Vladlen\u0026rsquo;s talk \u0026ldquo;Doing (Good) Research\u0026rdquo;, both worth watching in their entirety!\nProblem Progressing from undergrad into research can seem to flip any STEM field from a science into an art: undergrads identify solutions to problems versus researchers who identify problems worth solving. The right problem is ill-defined, yet critical to identify.\nDefine \u0026ldquo;right\u0026rdquo; Richard: right is simply subjective, up to you.\nVladlen: right should:\n Contribute to the progress and well-being of humanity, should be useful. Maximize your contribution to the research community and society (not vice versa) by  inspiring the community; shifting the way the community thinks about existing problems (or new ones); be a methodical evaluation of methods to demonstrate which should be used when; be a dataset, benchmark, or open source code people find useful.   How to work on the \u0026ldquo;right\u0026rdquo; problems  Create a long term vision to guide yourself (not a random walk), have high-level meta goals.  Ask yourself: what fields or applications do you find interesting or meaningful and why? You\u0026rsquo;ll only really succeed in work that you enjoy. Doing meaningful work helps avoid burnout.  Understand what components are needed for your high-level goals to become a reality. Understand the state-of-the-art for each component by reading and doing (explained later).  Analyze bottlenecks. Understand what you could do to benefit the collective progress of the community.  Solve a component\u0026rsquo;s problem if the time is right.  Too early = not sufficient tools to attack the problem well yet, will make little headway. Too late = you won\u0026rsquo;t contribute to affecting the research community\u0026rsquo;s direction much, improvements will be too incremental.   Understand by reading  Read a lot. Read critically. When reading about a new method, ask:  What are its limitations and assumptions, when will it break? What gaps remain?  Reading critically enables new ways of thinking about methods. Otherwise you won\u0026rsquo;t make big changes, you\u0026rsquo;ll merely extend the old with incremental improvements. So tolerate some subjective uncertainty about the \u0026ldquo;accepted methods\u0026rdquo;. Be sufficiently skeptical to perceive flaws and see what others missed. It\u0026rsquo;s OK to be (justifiably) contrarian! Inform your colleagues about your research interests. They\u0026rsquo;ll give you greater observability of the relevant papers you should read (but missed).  Understand by doing  Re-implementation is the best way to understand a method. You\u0026rsquo;ll discover details you can\u0026rsquo;t glean from just reading, and develop deeper insight. Implementation helps observe things you never set out to discover. And by following the scientific method down this rabbit hole, you can then go about testing assumptions about what generated the effect you observed. Research then begets research: the more things you try, the more you observe, the more things you want to try next etc, the more people will want to collaborate with you, and invite you to things, and tell you about things you didn\u0026rsquo;t otherwise know. Be methodical when (re-)implementing: swap a method\u0026rsquo;s subcomponents in and out in a controlled way to understand which components were critical.  Understand by writing  Write down your ideas + experimental methods early. This makes clear what your assumptions are and what gaps remain to be filled. You can also share this (more concrete) document of your ideas with your peers.  Work ethic  Luck favors the prepared! Research is 99% perspiration, 1% inspiration. Newton thought if people just worked as hard as he did, they\u0026rsquo;d get the same result. Great researchers always think about problems, even when they\u0026rsquo;re away from work. Set aside quiet time reflection time for “great thoughts” like Friday afternoon to:  Think higher-level about where you are heading, is this the right direction? Don\u0026rsquo;t cling to bad ideas too long, know when to walk away / bury work.  Have 10\u0026ndash;20 problems in your head at any one time, so you don\u0026rsquo;t stay \u0026ldquo;stuck\u0026rdquo; in one problem. When a clue comes along for one of them, focus on it. Have an open door policy (need to collaborate), it\u0026rsquo;s better in the long run. Learn how to communicate your ideas in a formal and casual way. Be on the lookout for collaborators. Quality over quantity (do not add noise to conferences or arXiv, or worse: mislead).  Contribution is the goal, the publication process is a liability.  Confidence is necessary: it is important that you believe you can do great work.  Challenging yourself  Ask yourself: if what I\u0026rsquo;m doing is not important, and not likely to lead to important things in the future, why am I working on it?  Caveat: not to say adopt \u0026ldquo;Nobel prize winner syndrome\u0026rdquo; of only working on \u0026ldquo;great problems\u0026rdquo; and getting nowhere. Instead, work on small acorns that have the potential to grow into mighty oaks.  Ask yourself: what are the most important problems in my field? Be willing to accept change, don\u0026rsquo;t stick to a particular method.  Warnings  Sometimes the people around you cannot see you\u0026rsquo;re doing great work.  A printer-friendly version of this post is here.\n","date":1635206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635206400,"objectID":"8db034acdc3224abf38808ba0cf1d04a","permalink":"https://rowanmcallister.github.io/post/research/","publishdate":"2021-10-26T00:00:00Z","relpermalink":"/post/research/","section":"post","summary":"Choosing the *right* research problem + how to succeed","tags":null,"title":"How to do research","type":"post"},{"authors":["Boris Ivanovic","Kuan-Hui Lee","Pavel Tokmakov","Blake Wulfe","Rowan McAllister","Adrien Gaidon","Marco Pavone"],"categories":null,"content":"","date":1619481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619481600,"objectID":"8f458a2b1a1f0cbc4370997c8ae9362d","permalink":"https://rowanmcallister.github.io/publication/haicu/","publishdate":"2021-04-27T00:00:00Z","relpermalink":"/publication/haicu/","section":"publication","summary":"Reasoning about the future behavior of other agents is critical to safe robot navigation. The multiplicity of plausible futures is further amplified by the uncertainty inherent to agent state estimation from data, including positions, velocities, and semantic class. Forecasting methods, however, typically neglect class uncertainty, conditioning instead only on the agent’s most likely class, even though perception models often return full class distributions. To exploit this information, we present HAICU, a method for heterogeneous-agent trajectory forecasting that explicitly incorporates agents’ class probabilities. We additionally present PUP, a new challenging real-world autonomous driving dataset, to investigate the impact of Perceptual Uncertainty in Prediction. It contains challenging crowded scenes with unfiltered agent class probabilities that reflect the long-tail of current state-of-the-art perception systems. We demonstrate that incorporating class probabilities in trajectory forecasting significantly improves performance in the face of uncertainty, and enables new forecasting capabilities such as counterfactual predictions.","tags":["Autonomous Vehicles"],"title":"Heterogeneous-Agent Trajectory Forecasting Incorporating Class Uncertainty","type":"publication"},{"authors":["Nicholas Rhinehart","Jeff He","Charles Packer","Matthew Wright","Rowan McAllister","Joseph Gonzalez","Sergey Levine"],"categories":null,"content":"","date":1618963201,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618963201,"objectID":"33769b280296f33a6a29a3d139c7d660","permalink":"https://rowanmcallister.github.io/publication/contingency/","publishdate":"2021-04-21T00:00:01Z","relpermalink":"/publication/contingency/","section":"publication","summary":"Humans have a remarkable ability to make decisions by accurately reasoning about future events, including the future behaviors and states of mind of other agents. Consider driving a car through a busy intersection, it is necessary to reason about the physics of the vehicle, the intentions of other drivers, and their beliefs about your own intentions. If you signal a turn, another driver might yield to you, or if you enter the passing lane, another driver might decelerate to give you room to merge in front. Competent drivers must plan how they can safely react to a variety of potential future behaviors of other agents before they make their next move. This requires contingency planning, explicitly planning a set of conditional actions that depend on the stochastic outcome of future events. Contingency planning outputs a policy that is a function of future timesteps and observations, whereas standard model predictive control-based planning outputs a sequence of future actions, which is equivalent to a policy that is only a function of future timesteps. In this work, we develop a general-purpose contingency planner that is learned end-to-end using high-dimensional scene observations and low-dimensional behavioral observations. We use a conditional autoregressive flow model to create a compact contingency planning space, and show how this model can tractably learn contingencies from behavioral observations. We developed a closed-loop control benchmark of realistic multi-agent scenarios in a driving simulator (CARLA), on which we compare our method to various noncontingent methods that reason about multi-agent future behavior, including several state-of-the-art deep learning-based planning approaches. We illustrate that these noncontingent planning methods fundamentally fail on this benchmark, and find that our deep contingency planning method achieves significantly superior performance.","tags":["Autonomous Vehicles","Planning"],"title":"Contingencies from Observations: Tractable Contingency Planning with Learned Behavior Models","type":"publication"},{"authors":["Tim Rudner","Vitchyr Pong","Rowan McAllister","Yarin Gal","Sergey Levine"],"categories":null,"content":"","date":1618963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618963200,"objectID":"82e24aa42db734293b21b3a81f1dddd0","permalink":"https://rowanmcallister.github.io/publication/goals/","publishdate":"2021-04-21T00:00:00Z","relpermalink":"/publication/goals/","section":"publication","summary":"While reinforcement learning algorithms provide automated acquisition of optimal policies, practical application of such methods requires a number of design decisions, such as manually designing reward functions that not only define the task, but also provide sufficient shaping to accomplish it. In this paper, we discuss a new perspective on reinforcement learning, recasting it as the problem of inferring actions that achieve desired outcomes, rather than a problem of maximizing rewards. To solve the resulting outcome-directed inference problem, we establish a novel variational inference formulation that allows us to derive a well-shaped reward function which can be learned directly from environment interactions. From the corresponding variational objective, we also derive a new probabilistic Bellman backup operator reminiscent of the standard Bellman backup operator and use it to develop an off-policy algorithm to solve goal-directed tasks. We empirically demonstrate that this method eliminates the need to design reward functions and leads to effective goal-directed behaviors.","tags":["Reinforcement Learning"],"title":"Outcome-Driven Reinforcement Learning via Variational Inference","type":"publication"},{"authors":["Angelos Filos","Panagiotis Tigas","Rowan McAllister","Nicholas Rhinehart","Sergey Levine","Yarin Gal"],"categories":null,"content":"","date":1593129600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593129600,"objectID":"f76a56afb594d0a3f687c1bb0a09c6a7","permalink":"https://rowanmcallister.github.io/publication/carnovel/","publishdate":"2020-06-26T00:00:00Z","relpermalink":"/publication/carnovel/","section":"publication","summary":"Out-of-training-distribution (OOD) scenarios are a common challenge of learning agents at deployment, typically leading to arbitrary deductions and poorly-informed decisions. In principle, detection of and adaptation to OOD scenes can mitigate their adverse effects. In this paper, we highlight the limitations of current approaches to novel driving scenes and propose an epistemic uncertainty-aware planning method, called robust imitative planning (RIP). Our method can detect and recover from some distribution shifts, reducing the overconfident and catastrophic extrapolations in OOD scenes. If the model's uncertainty is too great to suggest a safe course of action, the model can instead query the expert driver for feedback, enabling sample-efficient online adaptation, a variant of our method we term adaptive robust imitative planning (AdaRIP). Our methods outperform current state-of-the-art approaches in the nuScenes prediction challenge, but since no benchmark evaluating OOD detection and adaption currently exists to assess control, we introduce an autonomous car novel-scene benchmark, CARNOVEL, to evaluate the robustness of driving agents to a suite of tasks with distribution shifts.","tags":["Autonomous Vehicles"],"title":"Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?","type":"publication"},{"authors":["Amy Zhang","Rowan McAllister","Roberto Calandra","Yarin Gal","Sergey Levine"],"categories":null,"content":"","date":1592438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592438400,"objectID":"26c6495452d6301b8d38e8514d2f8d2a","permalink":"https://rowanmcallister.github.io/publication/dbc/","publishdate":"2020-06-18T00:00:00Z","relpermalink":"/publication/dbc/","section":"publication","summary":"We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that both provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.","tags":["Reinforcement Learning"],"title":"Learning Invariant Representations for Reinforcement Learning without Reconstruction","type":"publication"},{"authors":["Suneel Belkhale","Rachel Li","Gregory Kahn","Rowan McAllister","Roberto Calandra","Sergey Levine"],"categories":null,"content":"","date":1587600000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587600000,"objectID":"f249819bd4b58080e78a19725f59b8e3","permalink":"https://rowanmcallister.github.io/publication/meta-mbrl/","publishdate":"2020-04-23T00:00:00Z","relpermalink":"/publication/meta-mbrl/","section":"publication","summary":"Transporting suspended payloads is challenging for autonomous aerial vehicles because the payload can cause significant and unpredictable changes to the robot's dynamics. These changes can lead to suboptimal flight performance or even catastrophic failure. Although adaptive control and learning-based methods can in principle adapt to changes in these hybrid robot-payload systems, rapid mid-flight adaptation to payloads that have a priori unknown physical properties remains an open problem. We propose a meta-learning approach that ``learns how to learn'' models of altered dynamics within seconds of post-connection flight data. Our experiments demonstrate that our online adaptation approach outperforms non-adaptive methods on a series of challenging suspended payload transportation tasks.","tags":["Reinforcement Learning"],"title":"Model-Based Meta-Reinforcement Learning forFlight with Suspended Payloads","type":"publication"},{"authors":["Brijen Thananjeyan","Ashwin Balakrishna","Ugo Rosolia","Felix Li","Rowan McAllister","Joseph Gonzalez","Sergey Levine","Francesco Borrelli","Ken Goldberg"],"categories":null,"content":"","date":1559260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559260800,"objectID":"64538f234b1030a7d33cabe80e59706b","permalink":"https://rowanmcallister.github.io/publication/saved/","publishdate":"2019-05-31T00:00:00Z","relpermalink":"/publication/saved/","section":"publication","summary":"Reinforcement learning (RL) for robotics is challenging due to the difficulty in hand-engineering a dense cost function, which can lead to unintended behavior, and dynamical uncertainty, which makes it hard to enforce constraints during learning. We address these issues with a new model-based reinforcement learning algorithm, safety augmented value estimation from demonstrations (SAVED), which uses supervision that only identifies task completion and a modest set of suboptimal demonstrations to constrain exploration and learn efficiently while handling complex constraints. We derive iterative improvement guarantees for SAVED under known stochastic nonlinear systems. We then compare SAVED with 3 state-of-the-art model-based and model-free RL algorithms on 6 standard simulation benchmarks involving navigation and manipulation and 2 real-world tasks on the da Vinci surgical robot. Results suggest that SAVED outperforms prior methods in terms of success rate, constraint satisfaction, and sample efficiency, making it feasible to safely learn complex maneuvers directly on a real robot in less than an hour. For tasks on the robot, baselines succeed less than 5% of the time while SAVED has a success rate of over 75% in the first 50 training iterations.","tags":null,"title":"Safety Augmented Value Estimation from Demonstrations (SAVED): Safe Deep Model-Based RL for Sparse Cost Robotic Tasks","type":"publication"},{"authors":["Nicholas Rhinehart","Rowan McAllister","Kris Kitani","Sergey Levine"],"categories":null,"content":"","date":1557187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557187200,"objectID":"4314b09673cf171f2a6de1d76368f903","permalink":"https://rowanmcallister.github.io/publication/precog/","publishdate":"2019-05-07T00:00:00Z","relpermalink":"/publication/precog/","section":"publication","summary":"Forecasting the motion of multiple interacting vehicles. When one is autonmous, conditioning on its goals helps better-predict the motions of other vehicles.","tags":["Autonomous Vehicles"],"title":"PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings","type":"publication"},{"authors":["Rowan McAllister","Gregory Kahn","Jeff Clune","Sergey Levine"],"categories":null,"content":"","date":1545868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545868800,"objectID":"392cad5eb99b10f7e9a745b76f7adc1e","permalink":"https://rowanmcallister.github.io/publication/ood/","publishdate":"2018-12-27T00:00:00Z","relpermalink":"/publication/ood/","section":"publication","summary":"Deep learning provides a powerful tool for machine perception when the observations resemble the training data. However, real-world robotic systems must react intelligently to their observations even in unexpected circumstances. This requires a system to reason about its own uncertainty given unfamiliar, out-of-distribution observations. Approximate Bayesian approaches are commonly used to estimate uncertainty for neural network predictions, but can struggle with out-of-distribution observations. Generative models can in principle detect out-of-distribution observations as those with a low estimated density. However, the mere presence of an out-of-distribution input does not by itself indicate an unsafe situation. In this paper, we present a method for uncertainty-aware robotic perception that combines generative modeling and model uncertainty to cope with uncertainty stemming from out-of-distribution states. Our method estimates an uncertainty measure about the model's prediction, taking into account an explicit (generative) model of the observation distribution to handle out-of-distribution inputs. This is accomplished by probabilistically projecting observations onto the training distribution, such that out-of-distribution inputs map to uncertain in-distribution observations, which in turn produce uncertain task-related predictions, but only if task-relevant parts of the image change. We evaluate our method on an action-conditioned collision prediction task with both simulated and real data, and demonstrate that our method of projecting out-of-distribution observations improves the performance of four standard Bayesian and non-Bayesian neural network approaches, offering more favorable trade-offs between the proportion of time a robot can remain autonomous and the proportion of impending crashes successfully avoided.","tags":["Reinforcement Learning"],"title":"Robustness to Out-of-Distribution Inputs via Task-Aware Generative Uncertainty","type":"publication"},{"authors":["Kurtland Chua","Roberto Calandra","Rowan McAllister","Sergey Levine"],"categories":null,"content":"","date":1541116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541116800,"objectID":"9eb0854654ecbd931b95d5384b219ab5","permalink":"https://rowanmcallister.github.io/publication/pets/","publishdate":"2018-11-02T00:00:00Z","relpermalink":"/publication/pets/","section":"publication","summary":"Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).","tags":["Reinforcement Learning"],"title":"Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models","type":"publication"},{"authors":["Nicholas Rhinehart","Rowan McAllister","Sergey Levine"],"categories":null,"content":"","date":1539561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539561600,"objectID":"24431e94f69a8cbd95a0bb1775cb5c7c","permalink":"https://rowanmcallister.github.io/publication/dim/","publishdate":"2018-10-15T00:00:00Z","relpermalink":"/publication/dim/","section":"publication","summary":"Imitation Learning (IL) is an appealing approach to learn desirable autonomous behavior. However, directing IL to achieve arbitrary goals is difficult. In contrast, planning-based algorithms use dynamics models and reward functions to achieve goals. Yet, reward functions that evoke desirable behavior are often difficult to specify. In this paper, we propose \"Imitative Models\" to combine the benefits of IL and goal-directed planning. Imitative Models are probabilistic predictive models of desirable behavior able to plan interpretable expert-like trajectories to achieve specified goals. We derive families of flexible goal objectives, including constrained goal regions, unconstrained goal sets, and energy-based goals. We show that our method can use these objectives to successfully direct behavior. Our method substantially outperforms six IL approaches and a planning-based approach in a dynamic simulated autonomous driving task, and is efficiently learned from expert demonstrations without online data collection.  We also show our approach is robust to poorly-specified goals, such as goals on the wrong side of the road.","tags":["Autonomous Vehicles"],"title":"Deep Imitative Models for Flexible Inference, Planning, and Control","type":"publication"},{"authors":["Rowan McAllister","Carl Rasmussen"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"af2caa51d9b47076b5554063df582d0c","permalink":"https://rowanmcallister.github.io/publication/fpilco/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/fpilco/","section":"publication","summary":"We present a data-efficient reinforcement learning method for continuous stateaction systems under significant observation noise. Data-efficient solutions under small noise exist, such as PILCO which learns the cartpole swing-up task in 30s. PILCO evaluates policies by planning state-trajectories using a dynamics model. However, PILCO applies policies to the observed state, therefore planning in observation space. We extend PILCO with filtering to instead plan in belief space, consistent with partially observable Markov decisions process (POMDP) planning. This enables data-efficient learning under significant observation noise, outperforming more naive methods such as post-hoc application of a filter to policies optimised by the original (unfiltered) PILCO algorithm. We test our method on the cartpole swing-up task, which involves nonlinear dynamics and requires nonlinear control.","tags":["Reinforcement Learning","Planning"],"title":"Data-Efficient Reinforcement Learning in Continuous State-Action Gaussian-POMDPs","type":"publication"},{"authors":["Rowan McAllister","Yarin Gal","Alex Kendall","Mark van der Wilk","Amar Shah","Roberto Cipolla","Adrian Weller"],"categories":null,"content":"","date":1503100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503100800,"objectID":"5147407aa40769f729df444826aa7125","permalink":"https://rowanmcallister.github.io/publication/avsafety/","publishdate":"2017-08-19T00:00:00Z","relpermalink":"/publication/avsafety/","section":"publication","summary":"Autonomous vehicle (AV) software is typically composed of a pipeline of individual components, linking sensor inputs to motor outputs. Erroneous component outputs propagate downstream, hence safe AV software must consider the ultimate effect of each component's errors. Further, improving safety alone is not sufficient. Passengers must also *feel* safe to trust and use AV systems. To address such concerns, we investigate three under-explored themes for AV research; safety, interpretability, and compliance. *Safety* can be improved by quantifying the uncertainties of component outputs and propagating them forward through the pipeline. *Interpretability* is concerned with explaining what the AV observes and why it makes the decisions it does, building reassurance with the passenger. *Compliance* refers to maintaining some control for the passenger. We discuss open challenges for research within these themes. We highlight the need for concrete evaluation metrics, propose example problems, and highlight possible solutions.","tags":["Autonomous Vehicles"],"title":"Concrete Problems for Autonomous Vehicle Safety: Advantages of Bayesian Deep Learning","type":"publication"},{"authors":["Rowan McAllister"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"6aad632eb6eb4d81dc799d4ff1bf17a6","permalink":"https://rowanmcallister.github.io/publication/phd/","publishdate":"2017-05-01T00:00:00Z","relpermalink":"/publication/phd/","section":"publication","summary":"Applications to learn control of unfamiliar dynamical systems with increasing autonomy are ubiquitous. From robotics, to finance, to industrial processing, autonomous learning helps obviate a heavy reliance on experts for system identification and controller design. Often real world systems are nonlinear, stochastic, and expensive to operate (e.g. slow, energy intensive, prone to wear and tear). Ideally therefore, nonlinear systems can be identified with minimal system interaction. This thesis considers data efficient autonomous learning of control of nonlinear, stochastic systems. Data efficient learning critically requires probabilistic modelling of dynamics. Traditional control approaches use deterministic models, which easily overfit data, especially small datasets. We use probabilistic Bayesian modelling to learn systems from scratch, similar to the PILCO algorithm, which achieved unprecedented data efficiency in learning control of several benchmarks. We extend PILCO in three principle ways. First, we learn control under significant observation noise by simulating a filtered control process using a tractably analytic framework of Gaussian distributions. In addition, we develop the ‘latent variable belief Markov decision process’ when filters must predict under real-time constraints. Second, we improve PILCO’s data efficiency by directing exploration with predictive loss uncertainty and Bayesian optimisation, including a novel approximation to the Gittins index. Third, we take a step towards data efficient learning of high-dimensional control using Bayesian neural networks (BNN). Experimentally we show although filtering mitigates adverse effects of observation noise, much greater performance is achieved when optimising controllers with evaluations faithful to reality; by simulating closed-loop filtered control if executing closed-loop filtered control. Thus, controllers are optimised w.r.t. how they are used, outperforming filters applied to systems optimised by unfiltered simulations. We show directed exploration improves data efficiency. Lastly, we show BNN dynamics models are almost as data efficient as Gaussian process models. Results show data efficient learning of high-dimensional control is possible as BNNs scale to high-dimensional state inputs.","tags":null,"title":"Bayesian Learning for Data-Efficient Control","type":"publication"},{"authors":["Yarin Gal","Rowan McAllister","Carl Rasmussen"],"categories":null,"content":"","date":1466294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466294400,"objectID":"c7ee01b13488d6f9297c7dd9623af77d","permalink":"https://rowanmcallister.github.io/publication/dpilco/","publishdate":"2016-06-19T00:00:00Z","relpermalink":"/publication/dpilco/","section":"publication","summary":"Model-based reinforcement learning (RL) allows an agent to discover good policies with a small number of trials by generalising observed transitions. Data efficiency can be further improved with a probabilistic model of the agent’s ignorance about the world, allowing it to choose actions under uncertainty. Bayesian modelling offers tools for this task, with PILCO being a prominent example, achieving state-of-theart data efficiency on low dimensional RL benchmarks. But PILCO relies on Gaussian processes (GPs), which prohibits its applicability to problems that require a larger number of trials to be solved. Further, PILCO does not consider temporal correlation in model uncertainty between successive state transitions, which results in PILCO underestimating state uncertainty at future time steps. In this paper we extend PILCO’s framework to use Bayesian deep dynamics models with approximate variational inference, allowing PILCO to scale linearly with number of trials and observation space dimensionality. Using particle methods we sample dynamics function realisations, and obtain lower cumulative cost than PILCO. We give insights into the modelling assumptions made in PILCO, and show that moment matching is a crucial simplifying assumption made by the model. Our implementation can leverage GPU architectures, offering faster running time than PILCO, and will allow structured observation spaces to be modelled (images or higher dimensional inputs) in the future.","tags":null,"title":"Improving PILCO with Bayesian Neural Network Dynamics Models","type":"publication"},{"authors":["Thierry Peynot","Angela Lui","Rowan McAllister","Robert Fitch","Salah Sukkarieh"],"categories":null,"content":"","date":1410220800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1410220800,"objectID":"30c5e97300aab389a811a9150e0d5bba","permalink":"https://rowanmcallister.github.io/publication/mawson-jfr/","publishdate":"2014-09-09T00:00:00Z","relpermalink":"/publication/mawson-jfr/","section":"publication","summary":"Motion planning for planetary rovers must consider control uncertainty in order to maintain the safety of the platform during navigation. Modeling such control uncertainty is difficult due to the complex interaction between the platform and its environment. In this paper, we propose a motion-planning approach whereby the outcome of control actions is learned from experience and represented statistically using a Gaussian process regression model. This mobility prediction model is trained using sample executions of motion primitives on representative terrain, and it predicts the future outcome of control actions on similar terrain. Using Gaussian process regression allows us to exploit its inherent measure of prediction uncertainty in planning. We integrate mobility prediction into a Markov decision process framework and use dynamic programming to construct a control policy for navigation to a goal region in a terrain map built using an onboard depth sensor. We consider both rigid terrain, consisting of uneven ground, small rocks, and nontraversable rocks, and also deformable terrain. We introduce two methods for training the mobility prediction model from either proprioceptive or exteroceptive observations, and we report results from nearly 300 experimental trials using a planetary rover platform in a Mars-analogue environment. Our results validate the approach and demonstrate the value of planning under uncertainty for safe and reliable navigation.","tags":["Mawson the Robot"],"title":"Learned Stochastic Mobility Prediction for Planning with Control Uncertainty on Unstructured Terrain","type":"publication"},{"authors":["Rowan McAllister","Thierry Peynot","Robert Fitch","Salah Sukkarieh"],"categories":null,"content":"","date":1349568000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1349568000,"objectID":"b3fb5afc4488d633ceb3e93e4c0e7ac3","permalink":"https://rowanmcallister.github.io/publication/mawson-iros/","publishdate":"2012-10-07T00:00:00Z","relpermalink":"/publication/mawson-iros/","section":"publication","summary":"Motion planning for planetary rovers must consider control uncertainty in order to maintain the safety of the platform during navigation. Modelling such control uncertainty is difficult due to the complex interaction between the platform and its environment. In this paper, we propose a motion planning approach whereby the outcome of control actions is learned from experience and represented statistically using a Gaussian process regression model. This model is used to construct a control policy for navigation to a goal region in a terrain map built using an on-board RGB-D camera. The terrain includes flat ground, small rocks, and non-traversable rocks. We report the results of 200 simulated and 35 experimental trials that validate the approach and demonstrate the value of considering control uncertainty in maintaining platform safety.","tags":["Mawson the Robot"],"title":"Motion Planning and Stochastic Control with Experimental Validation on a Planetary Rover","type":"publication"},{"authors":["Rowan McAllister"],"categories":null,"content":"","date":1343779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1343779200,"objectID":"656dc45078960742f3bf0226a72e2834","permalink":"https://rowanmcallister.github.io/publication/mawson-masters/","publishdate":"2012-08-01T00:00:00Z","relpermalink":"/publication/mawson-masters/","section":"publication","summary":"Planetary rovers are required to safely navigate across unstructured and hazardous terrain with increasing levels of autonomy. Autonomy is necessary to react to impending danger because remote control has been proved challenging and dangerous for planetary rovers due to the large communication delays. Safety is especially a concern in space applications where a robot is unaccompanied throughout its entire mission. Unstructured terrain poses several types of hazards to a robot such as getting stuck, toppling or scraping against rocks. In a dense collection of localised hazards, platform safety is very sensitive to deviations from an intended path. To maintain the safety of the platform during navigation, planetary rovers must consider control uncertainty during motion planning. Thus, not only does the system need to make predictions of action outcomes, it also needs to estimate the accuracy of these predictions. The aim of this research is to provide planetary rovers with the ability to plan motions to goal regions that optimise the safety of the platform by including information about the accuracy of its controls. Modelling such control uncertainty is diffcult due to the complex interaction between the platform and its environment. In this thesis, we propose an approach to learn the outcome of control actions from experience, represented statistically using a Gaussian Process regression model. This model is incorporated explicitly in the planning process using dynamic programming to construct a control policy for navigation to a goal region. Motion planning strategies are considered that take into account different types of uncertainties, including uncertainty in distance, heading and yaw of the platform, across various motion primitives. The approach is implemented on a holonomic rover with six wheels and a Rocker-bogie frame and tested on a Mars analogue terrain that includes flat ground, small rocks, and non-traversable rocks. Planning is computed over a terrain map built using an on-board RGB-D camera. We report the results of 200 simulated and 95 experimental trials that validate the approach. These results demonstrate that explicitly incorporating knowledge of control uncertainty into the motion planning process increases platform safety by decreasing the likelihood of the rover getting stuck and reducing the cost accumulated over the executed path. Accounting for heading uncertainty resulted in the most significant increase in platform safety.","tags":["Mawson the Robot"],"title":"Motion Planning and Stochastic Control with Experimental Validation on a Planetary Rover","type":"publication"},{"authors":["Rowan McAllister"],"categories":null,"content":"","date":1257379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1257379200,"objectID":"e9c4264ea1e976ea01194cfffc8c9354","permalink":"https://rowanmcallister.github.io/publication/undergrad/","publishdate":"2009-11-05T00:00:00Z","relpermalink":"/publication/undergrad/","section":"publication","summary":"Self-Reconfiguring Robots (SRR) are composed of many modules that have the ability to autonomously attach and detach, enabling adaptation to a variety of tasks in unknown surroundings. In order to change shape into a particular form, an SRR must plan a sequence of module movements. This reconfiguration problem is challenging because of the many mechanical degrees of freedom and the resultant large number of possible SRR configurations which contribute to a vast and high-dimensional search space. To support the operation of an SRR in a practical environment, reconfiguration planners must satisfy a number of properties including decentralized computation, parallel motion of modules, real-time execution and planning in the native kinematic space of the module mechanism. The ultimate solution would be a general planner that solves reconfigurations of arbitrary module designs in this way. This thesis has taken a step in this direction of generality by developing a reconfiguration planner of the 3R module that is easily instantiable to other module types. The planner is scalable and decentralized, and considers the native kinematics of a module. It demonstrates the ability to coordinate the parallel motion of modules and executes in real-time. The thesis presents both centralized and decentralized implementations of the algorithm along with performance evaluation for several reconfiguration examples, as well as an analysis of achievable SRR reconfigurations.","tags":["Modular Robots"],"title":"Autonomous Reconfiguration Planning in Modular Robots","type":"publication"}]